{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d65a9c6",
   "metadata": {},
   "source": [
    "# Library of functions for the AFM-SINDy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea1c65",
   "metadata": {},
   "source": [
    "### List of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import Image\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from pysindy.utils import lorenz, lorenz_control, enzyme\n",
    "import pysindy as ps\n",
    "import cvxpy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import dill\n",
    "import os\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa36ed",
   "metadata": {},
   "source": [
    "## Miscellaneous Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46084457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_style(style_num = 17):\n",
    "    # Version 2.0: This function sets the style of the images that are generated by matplotlib. \n",
    "    # New in Version 2.0: Added functionality to put back the plotting style to 'default' when styly_num = 'default'.\n",
    "    plt.style.use('default')\n",
    "    plt.style.use(plt.style.available[style_num])\n",
    "\n",
    "    if style_num == 'default': # New in version 2.0\n",
    "        plt.style.use('default')\n",
    "\n",
    "def convert_to_table(equation_string):\n",
    "    #Version 2.0: This function gets the string of an equation found by SINDy and it splits the string in the different terms of the equation. \n",
    "    #             Whenever the function sees a + that is followed by blank spaces like \" + \", then it splits the string to generate a term. \n",
    "    #             However, whereever the functions sees a + without a space like in \"(1+x)^-8\", this is not splitted, as is considered as a single term.\n",
    "\n",
    "    #Added in version 2: The function now can distinguish terms like \"(1+x)^-8\" as it does not split strings where the + is immidiatly\n",
    "    #                    followed by strings different that a blank space.\n",
    "\n",
    "    equation_terms = equation_string\n",
    "    separated_terms = re.split(r' \\+ ', equation_terms)  # Split on space-surrounded plus signs\n",
    "\n",
    "    # Split each term into components based on spaces, then filter out any empty strings\n",
    "    term_components = [term.split(' ') for term in separated_terms]\n",
    "    cleaned_components = [[component for component in components if component] for components in term_components]\n",
    "\n",
    "    # Create a dictionary to hold the coefficient and variable parts of each term\n",
    "    coefficients_variables_dict = {}\n",
    "    for term in cleaned_components:\n",
    "        variable_part = '*'.join(term[1:])  # Join all parts except the coefficient with '*'\n",
    "        coefficients_variables_dict[variable_part] = float(term[0])  # Convert coefficient to float and add to dict\n",
    "\n",
    "    # Convert the dictionary to a pandas Series and return\n",
    "    equation_series = pd.Series(coefficients_variables_dict)\n",
    "    return equation_series\n",
    "\n",
    "def score_tables(table_found, table_ref):\n",
    "    #Version 1: This function analyzes an equation that has been found by SINDy and that has been splitted in a table using convert_to_table().\n",
    "    #           The function compares the found equation with the reference equation to verify that the found equation has exacly the same terms\n",
    "    #           as the reference equation. If both have the same term, then the function return the string \"correct\". \n",
    "    #           Important: The coefficients are not being checked by this function, as these are found by the .fit() method of SINDy.\n",
    "\n",
    "    # Parameters: \"table_found\": Is the found equation in its table form.\n",
    "    #             \"table_ref\": Is the equation used as a reference. This must have the same terms that the objective equation that SINDy is trying to find. \n",
    "    \n",
    "    for var, val in table_found.items():\n",
    "        if var not in table_ref:\n",
    "            return('not correct')\n",
    "    for var, val in table_ref.items():\n",
    "        if var not in table_found:\n",
    "            return('not correct')\n",
    "    return ('correct')\n",
    "    \n",
    "def is_found_eq_same_as_ref_eq(equation, input_dict, should_print = False):\n",
    "    #Version 1: This function uses the convert_to_table() and the score_tables() to conveniently show if the equation used as reference,\n",
    "    #           matches the equation found by SINDy. If it matches, it return the string \"correct\"\n",
    "    #Parameters: \"equation\": Is the string of one single equation that SINDy has found. \n",
    "    #            \"input_dict\": Is the referece equation that one wants to find written in a dictionary. \n",
    "    #Example: If the equation is  (y)' = -0.995 x + -0.298 y + -0.995 x^3 + 0.199 cos(1*phase), then input_dict2 = {'y': -0.298, 'x':-1, 'x^3': -0.995, 'cos(1*t)': 0.199}. \n",
    "\n",
    "    tab = convert_to_table(equation)\n",
    "    table_ref = pd.Series(input_dict)\n",
    "    result = score_tables(tab, table_ref)\n",
    "\n",
    "    if should_print:\n",
    "        print('Equation used as reference has:')\n",
    "        print(table_ref)\n",
    "        print('Temp equation found by SINDy has:')\n",
    "        print(tab)\n",
    "        print(' ')\n",
    "        print('Therefore, the current found equation is: ' + result)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc78e838",
   "metadata": {},
   "source": [
    "## Functions to generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b01de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_different_trajectories(dynamical_system, num_trajectories, t_train, init_cond_range, DOF, noise_level, method = 'RK45', parameters=None, noisy_trajectories=False, add_ivp_time_array=False):\n",
    "    # Version 4.0: This function takes a defined dynamical system and it generates multiple trajectories of synthetic data that starts\n",
    "    # from different initial conditions.\n",
    "    # Added in version 2: This function now creates a solution_ivp varible to later split x_train = solution_ivp.y.T & t_train = solution_ivp.t\n",
    "    # Added in version 3: Now the function can generate different random init_cond within a range. Now the ranges can be written separately for disp, vel and phase.\n",
    "    # Added in version 4: Now the function also stores every random initial condition used to generate the different trajectories. Also progress bar was added.\n",
    "\n",
    "    t_train_span = (t_train[0], t_train[-1])\n",
    "    trajectories_list = []\n",
    "    init_cond_list = []\n",
    "    for trajectory in tqdm(range(num_trajectories), desc=\"Generating Trajectories\"):     # For loop with \"tqdm\" to add a progress bar\n",
    "        init_cond_disp = np.random.uniform(init_cond_range[0][0], init_cond_range[0][1])\n",
    "        init_cond_vel = np.random.uniform(init_cond_range[1][0], init_cond_range[1][1])\n",
    "        init_cond_phase = np.random.uniform(init_cond_range[2][0], init_cond_range[2][1])\n",
    "\n",
    "        if DOF == 1:\n",
    "            init_cond = np.array([init_cond_disp])\n",
    "        elif DOF == 2:\n",
    "            init_cond = np.array([init_cond_disp, init_cond_vel])\n",
    "        elif DOF == 3:\n",
    "            init_cond = np.array([init_cond_disp, init_cond_vel, init_cond_phase])\n",
    "\n",
    "        init_cond_list.append(init_cond)\n",
    "\n",
    "        if parameters is None:\n",
    "            solution_ivp = solve_ivp(dynamical_system, t_train_span, init_cond, t_eval=t_train, rtol=1e-6, atol=1e-8, method=method, dense_output=True)\n",
    "        else:\n",
    "            solution_ivp = solve_ivp(dynamical_system, t_train_span, init_cond, t_eval=t_train, args=parameters, rtol=1e-6, atol=1e-8, method=method, dense_output=True)\n",
    "        \n",
    "        x_train = solution_ivp.y.T\n",
    "        t_train_ivp = solution_ivp.t\n",
    "\n",
    "        if noisy_trajectories:\n",
    "            x_train = add_gaussian_noise_fixed_std(x_train, noise_level)\n",
    "        \n",
    "        if add_ivp_time_array:\n",
    "            x_train = np.vstack((x_train[:,0], x_train[:,1], t_train)).T\n",
    "\n",
    "        trajectories_list.append(x_train)\n",
    "\n",
    "    return trajectories_list, init_cond_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade508d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(x_train, noise_level):\n",
    "    # Version 2.0: This function adds Gaussian noise to the training data.\n",
    "    # Parameters: x_train: The original training data & noise_level: The desired noise level as a percentage.    \n",
    "    # Returns: A new dataset with added Gaussian noise.\n",
    "    # New in Version 2: Adds Gaussian noise only to displacement and velocity (columns 0 and 1)\n",
    "    \n",
    "    x_noisy = x_train.copy()\n",
    "    std_per_feature = np.std(x_train[:, :2], axis=0)  # Only displacement and velocity\n",
    "    noise_std = std_per_feature * (noise_level / 100.0)\n",
    "    noise = np.random.normal(0, noise_std, (x_train.shape[0], 2))\n",
    "    \n",
    "    x_noisy[:, :2] += noise  # Apply noise only to first two columns\n",
    "    return x_noisy\n",
    "\n",
    "def add_gaussian_noise_fixed_std(x_train, std=1e-5):\n",
    "    # Version 1: Adds Gaussian noise with mean = 0 and fixed std to only displacement and velocity\n",
    "    # Parameters: x_train: The original training data\n",
    "    x_noisy = x_train.copy()\n",
    "    noise = np.random.normal(loc=0.0, scale=std, size=(x_train.shape[0], 2))\n",
    "    x_noisy[:, :2] += noise  # Only add noise to columns 0 and 1\n",
    "    return x_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f364b",
   "metadata": {},
   "source": [
    "### Functions to treat the data with units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_AFM_displacement(afm_disp_array, eta_star, tau_array, omega_0):\n",
    "\n",
    "    true_displacement = afm_disp_array*eta_star\n",
    "    true_time = tau_array/omega_0\n",
    "\n",
    "    return true_displacement, true_time\n",
    "\n",
    "def get_true_AFM_velocity(afm_vel_array, eta_star, tau_array, omega_0):\n",
    "\n",
    "    true_velocity = afm_vel_array*(eta_star*omega_0)\n",
    "    true_time = tau_array/omega_0\n",
    "\n",
    "    return true_velocity, true_time\n",
    "\n",
    "def get_true_AFM_Tip_Sample_Force(afm_force_array, eta_star, tau_array, omega_0, cant_density, cant_width, cant_thickness, cant_length):\n",
    "\n",
    "    true_tip_sample_force = afm_force_array * ((cant_density * cant_width * cant_thickness * ((eta_star * 1e-9) ** 3) * (omega_0 ** 2)) / cant_length)\n",
    "\n",
    "    true_time = tau_array/omega_0\n",
    "\n",
    "    return true_tip_sample_force, true_time\n",
    "\n",
    "def calculate_AFM_DMT_Force(eta_1, a0, C1, C2):\n",
    "#   Version 1: This functions simulates the F_ts of a DMT model when a0, C1 and C2 are known. \n",
    "#              This function it is ment to be used with the function calculate_true_F_ts_DMT()    \n",
    "    \n",
    "    if 1-eta_1 <= a0: \n",
    "        F_ts_temp = (C1/(a0**2)) + C2* (a0-(1 - eta_1))**1.5 #Repulsive regime\n",
    "    else: \n",
    "        F_ts_temp = (C1/(1-eta_1)**(2))  #Attractive regime Force\n",
    "\n",
    "    return F_ts_temp \n",
    "\n",
    "def calculate_true_F_ts_DMT(sim_disp_data, a0, C1, C2):\n",
    "    #Version 1.0: This function takes the array of simulated data and calculates the Tip-Sample Force using the DMT model.\n",
    "\n",
    "    #Parameters: - sim_disp_data: Synthetic data generated with the function AFM_w_DMT().\n",
    "    #                             It can be either a single measurement in an np.array or multiple trajectories in a list.\n",
    "    #            - C1, C2 & a0: Float numbers containing the parameters C1, C2 and a0 of DMT model.\n",
    "    if isinstance(sim_disp_data, np.ndarray):\n",
    "        F_ts_true_array = np.zeros_like(sim_disp_data[:,0],dtype=float)\n",
    "        for i in range(len(sim_disp_data)):\n",
    "            x_elem = sim_disp_data[i, 0]\n",
    "            F_ts_temp = calculate_AFM_DMT_Force(eta_1=x_elem, a0=a0, C1=C1, C2=C2)\n",
    "            F_ts_true_array[i] = F_ts_temp\n",
    "        return F_ts_true_array \n",
    "\n",
    "    if isinstance(sim_disp_data, list):\n",
    "        F_ts_true_list = []\n",
    "        for trajectory in sim_disp_data:\n",
    "            F_ts_true_array = np.zeros_like(trajectory[:,0], dtype=float)\n",
    "            for i in range(len(trajectory)):\n",
    "                x_elem = trajectory[i, 0]\n",
    "                F_ts_temp = calculate_AFM_DMT_Force(eta_1=x_elem, a0=a0, C1=C1, C2=C2)\n",
    "                F_ts_true_array[i] = F_ts_temp\n",
    "            F_ts_true_list.append(F_ts_true_array.copy())\n",
    "\n",
    "        return F_ts_true_list\n",
    "    \n",
    "def calculate_AFM_DMT_viscoelas_damp_Force(eta_1, eta_2, a0, C1, C2, C3):\n",
    "#   Version 1: This functions simulates the F_ts of a DMT model when a0, C1, C2 and C3 are known. \n",
    "#              This function it is ment to be used with the function calculate_true_F_ts_DMT_viscoelast_damp()\n",
    "    \n",
    "    if 1-eta_1 <= a0: \n",
    "        F_ts_temp = (C1/(a0**2)) + C2* (a0-(1 - eta_1))**1.5 - C3*(((a0-(1 - eta_1))**(0.5))*eta_2) #Repulsive regime\n",
    "    else: \n",
    "        F_ts_temp = (C1/(1-eta_1)**(2))  #Attractive regime Force\n",
    "\n",
    "    return F_ts_temp \n",
    "\n",
    "def calculate_true_F_ts_DMT_viscoelast_damp(sim_disp_data, a0, C1,C2,C3):\n",
    "    #Version 1.0: This function takes the array of simulated data and calculates the Tip-Sample Force using the DMT model.\n",
    "\n",
    "    #Parameters: - sim_disp_data: Synthetic data generated with the function AFM_w_Lennard_Jones().\n",
    "    #                             It can be either a single measurement in an np.array or multiple trajectories in a list.\n",
    "    #            - C1, C2 & a0: Float numbers containing the parameters C1, C2 and a0 of DMT model with Kelvin Voigt\n",
    "    if isinstance(sim_disp_data, np.ndarray):\n",
    "        F_ts_true_array = np.zeros_like(sim_disp_data[:,0],dtype=float)\n",
    "        for i in range(len(sim_disp_data)):\n",
    "            x_elem = sim_disp_data[i, 0]\n",
    "            y_elem = sim_disp_data[i, 1]\n",
    "            F_ts_temp = calculate_AFM_DMT_viscoelas_damp_Force(eta_1=x_elem, eta_2=y_elem, a0=a0, C1=C1, C2=C2, C3=C3)\n",
    "            F_ts_true_array[i] = F_ts_temp\n",
    "        return F_ts_true_array \n",
    "\n",
    "    if isinstance(sim_disp_data, list):\n",
    "        F_ts_true_list = []\n",
    "        for trajectory in sim_disp_data:\n",
    "            F_ts_true_array = np.zeros_like(trajectory[:,0], dtype=float)\n",
    "            for i in range(len(trajectory)):\n",
    "                x_elem = trajectory[i, 0]\n",
    "                y_elem = trajectory[i, 1]  # This line was missing\n",
    "                F_ts_temp = calculate_AFM_DMT_viscoelas_damp_Force(\n",
    "                    eta_1=x_elem, eta_2=y_elem, a0=a0, C1=C1, C2=C2, C3=C3)\n",
    "                F_ts_true_array[i] = F_ts_temp\n",
    "            F_ts_true_list.append(F_ts_true_array.copy())  # .copy() to avoid overwriting\n",
    "        return F_ts_true_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbf85e",
   "metadata": {},
   "source": [
    "## Functions to generate candidate functions\n",
    "\n",
    "Here functions are being defined to later be used and called by the function that generates the library itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f96113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sine(f):\n",
    "    return lambda x: np.sin(f*x)\n",
    "\n",
    "def make_sine_name(f):\n",
    "    return lambda x: f\"sin({f}*{x})\"\n",
    "#-----------------------------------\n",
    "def make_cosine(f):\n",
    "    return lambda x: np.cos(f*x)\n",
    "\n",
    "def make_cosine_name(f):\n",
    "    return lambda x: f\"cos({f}*{x})\"\n",
    "#-----------------------------------\n",
    "def make_poly(degree):\n",
    "    if degree == 1:\n",
    "        return lambda x: x\n",
    "    else:\n",
    "        return lambda x: x**degree\n",
    "\n",
    "def make_poly_name(degree):\n",
    "    if degree == 1:\n",
    "        return lambda x: x\n",
    "    else:\n",
    "        return lambda x: x+'^'+str(degree)\n",
    "#-----------------------------------    \n",
    "def make_poly_comb(DOF): \n",
    "    if DOF == 2:\n",
    "        return lambda x,y: x*y\n",
    "    if DOF == 3:\n",
    "        return lambda x,y,z: x*y*z\n",
    "\n",
    "def make_poly_comb_name(DOF):\n",
    "    if DOF == 2:\n",
    "        return lambda x,y: x+y\n",
    "    if DOF == 3:\n",
    "        return lambda x,y,z: x+y+z\n",
    "#-----------------------------------\n",
    "def make_poly_frac(degree):\n",
    "    return lambda x: x**(-degree)\n",
    "\n",
    "def make_poly_frac_name(degree):\n",
    "    return lambda x: x+'^'+str(-degree)\n",
    "#-----------------------------------\n",
    "def make_denom_cosine(f, degree):\n",
    "    return lambda x: (np.cos(f*x))**(-degree)\n",
    "\n",
    "def make_denom_cosine_name(f, degree):\n",
    "    return lambda x: f\"cos({f}*{x})\"+'^'+str(-degree)\n",
    "#-----------------------------------\n",
    "def make_denom_sine(f, degree):\n",
    "    return lambda x: (np.sin(f*x))**(-degree)\n",
    "\n",
    "def make_denom_sine_name(f, degree):\n",
    "    return lambda x: f\"sin({f}*{x})\"+'^'+str(-degree)\n",
    "#-----------------------------------\n",
    "def make_AFM_LJ_func(degree): # This function is to generate the Lennard Jones term (1-x)^-(degree)\n",
    "    return lambda x: (1-x)**(-degree)\n",
    "\n",
    "def make_AFM_LJ_func_name(degree):\n",
    "    return lambda x:f\"({1}-{x})\"+'^'+str(-degree)\n",
    "#-----------------------------------\n",
    "def make_1_over_eta_func(degree): # This function is to generate (eta)^-(degree)\n",
    "    return lambda x: (x)**(-degree)\n",
    "\n",
    "def make_1_over_eta_func_name(degree):\n",
    "    return lambda x:f\"({x})\"+'^'+str(-degree)\n",
    "#-----------------------------------\n",
    "def make_AFM_LJ_pos_degree_func(degree):   #This function is to generate the term z^degree =  (1-x)^(degree)\n",
    "    return lambda x: (1-x)**(degree)\n",
    "\n",
    "def make_AFM_LJ_pos_degree_name(degree):\n",
    "    return lambda x: f\"({1}-{x})\"+'^'+str(degree)\n",
    "#-----------------------------------\n",
    "\n",
    "def make_AFM_LJ_damp_func(degree, degree_2): #This function is to generate the LJ term ((1-x))^(0.degree))*y with a damping.\n",
    "    return lambda x,y: ((1 - x)**(degree))*(y**degree_2) \n",
    "\n",
    "def make_AFM_LJ_damp_name(degree, degree_2):\n",
    "    if degree == 1 and degree_2 == 1:\n",
    "        return lambda x,y: f\"(({1}-{x})\"+f\")*{y}\"\n",
    "    elif degree == 1:\n",
    "        return lambda x,y: f\"(({1}-{x})\"+f\")*{y}\"+'^'+str(degree_2) \n",
    "    elif degree_2 == 1:\n",
    "        return lambda x,y: f\"(({1}-{x})\"+'^'+str(degree)+f\")*{y}\"\n",
    "    else:\n",
    "        return lambda x,y: f\"(({1}-{x})\"+'^'+str(degree)+f\")*{y}\"+'^'+str(degree_2) \n",
    "#-----------------------------------\n",
    "\n",
    "def make_exp_damping_func(z_b, degree):\n",
    "    return lambda x,y: (y**degree)*(np.exp((1-x)/(z_b)))\n",
    "\n",
    "def make_exp_damping_name(z_b, degree):\n",
    "    if degree == 1:\n",
    "        return lambda x,y: f\"{y}\"+\"*exp\"+f\"({1}-{x}/{z_b})\"\n",
    "    else:\n",
    "        return lambda x,y: f\"{y}\"+'^'+str(degree)+\"*exp\"+f\"({1}-{x}/{z_b})\"   \n",
    "#-----------------------------------\n",
    "def vdw(x,a0,degree):\n",
    "\n",
    "    cand_func_array=np.zeros_like(x,dtype=float)\n",
    "    for ii in range(len(x)):\n",
    "        if 1 - x[ii]  < a0:\n",
    "            cand_func_array[ii]=0\n",
    "        else:\n",
    "            cand_func_array[ii]=(1-x[ii])**(-degree)\n",
    "    return cand_func_array\n",
    "\n",
    "def make_AFM_DMT_att_func(x, a0, degree): #This function is to generate the DMT term (1-x))^-(degree)\n",
    "    return lambda x: vdw(x,a0,degree)\n",
    "\n",
    "def make_AFM_DMT_att_name(a0, degree):\n",
    "    return lambda x: f\"({1}-{x})\"+'^'+str(-degree) \n",
    "#-----------------------------------\n",
    "\n",
    "def dmt_rep(x,a0,degree):\n",
    "\n",
    "    cand_func_array=np.zeros_like(x,dtype=float)\n",
    "    for ii in range(len(x)):\n",
    "        if 1 - x[ii]  < a0:\n",
    "            cand_func_array[ii]=np.power(a0 - 1 + x[ii], degree)      \n",
    "        else:\n",
    "            cand_func_array[ii]= 0\n",
    "    return cand_func_array  \n",
    "\n",
    "def make_AFM_DMT_rep_func(x, a0, degree): #This function is to generate the DMT term (a0-(1-x))^-(degree)\n",
    "    return lambda x: dmt_rep(x,a0,degree)\n",
    "\n",
    "def make_AFM_DMT_rep_name(a0, degree):\n",
    "    return lambda x:  f\"({a0}-{1}+{x})\" +'^'+str(degree) \n",
    "#-----------------------------------\n",
    "\n",
    "def DMT_rep_viscoel_damp(x, y, a0, degree, degree_2):\n",
    "#   Version 2: This function generates the discontinous function for the viscoelastic term in a DMT model \n",
    "#              of the shape ((a0-1-e1)^degree)*(e2^degree_2).\n",
    "#   New in Version 2: added the funcitonality for 'y' to have a different exponent than 1 by introducing\n",
    "#                     the parameter degree_2\n",
    "#   Parameters : a_0 is the intermolecular distance value\n",
    "#                degree is the exponent that goes with 'x' variable.\n",
    "#                degree_2 is the exponent that goes with 'y' variable.\n",
    "\n",
    "    cand_func_array=np.zeros_like(x,dtype=float)\n",
    "    for ii in range(len(x)):\n",
    "        if 1 - x[ii]  < a0:\n",
    "            cand_func_array[ii]=((a0 - 1 + x[ii])**(degree))*(y[ii]**degree_2) #here before it was hardcoded so that degree was always 0.5\n",
    "        else:\n",
    "            cand_func_array[ii]= 0\n",
    "    return cand_func_array  \n",
    "\n",
    "def make_AFM_DMT_viscoel_damp_func(x, y, a0, degree, degree_2):                     \n",
    "    return lambda x,y: DMT_rep_viscoel_damp(x, y, a0, degree, degree_2)\n",
    "\n",
    "def make_AFM_DMT_viscoel_damp_name(a0, degree, degree_2):\n",
    "    if degree == 1 and degree_2 == 1:\n",
    "        return lambda x,y: f\"(({a0}-1+{x})\"+f\")*{y}\"\n",
    "    elif degree == 1:\n",
    "        return lambda x,y: f\"(({a0}-1+{x})\"+f\")*{y}\"+'^'+str(degree_2) \n",
    "    elif degree_2 == 1:\n",
    "        return lambda x,y: f\"(({a0}-1+{x})\"+'^'+str(degree)+f\")*{y}\"\n",
    "    else:\n",
    "        return lambda x,y: f\"(({a0}-1+{x})\"+'^'+str(degree)+f\")*{y}\"+'^'+str(degree_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0828cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ranges(value):\n",
    "#     Version 2.0: To be used in the function create_candidate_func_w_func_names(). \n",
    "#     It Converts an input value to a set of integers (handles both single integers and tuples/ranges)\n",
    "#     The idea is to be able to generate only the powers of a candidate function that are needed. In a polynomial library for instance\n",
    "#     To generate only powers of 2, 5,6,7 10 can be written as poly_degrees=[2, (5,7), 10].\n",
    "#     New in version 2.0: Modified to handle floats in addition to integers and ranges.\n",
    "\n",
    "    result = set()\n",
    "    if isinstance(value, (int, float)):\n",
    "        result.add(value)  # If it's an integer or float, add it to the set\n",
    "    elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "        start, end = value\n",
    "        if all(isinstance(v, (int, float)) for v in (start, end)):\n",
    "            # If both elements are int or float, generate the range.\n",
    "            # Using a step of 1 for int range, approximate steps for float range.\n",
    "            if isinstance(start, int) and isinstance(end, int):\n",
    "                result.update(range(start, end + 1))\n",
    "            else:\n",
    "                # Generate numbers from start to end, with an increment that approximates integer steps.\n",
    "                increment = 1 if start < end else -1\n",
    "                while start <= end:\n",
    "                    result.add(start)\n",
    "                    start += increment\n",
    "        else:\n",
    "            raise ValueError(f\"Both elements in the tuple/list must be integers or floats. Received: {value}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid input, expected int, float, or tuple/list with two integers or floats. Received: {value}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649526dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidate_func_w_func_names(DOF, poly_degrees=None, AFM_LJ_degrees=None, DMT_rep_degrees=None, DMT_att_degrees=None, n_frequencies=None, AFM_amp=None, a0_val = None, DMT_data = None,\n",
    "                                       poly=True, poly_frac=False, sin=False, cos=False, exp_damp = False, z_b_val=None, exp_damp_degrees = None,\n",
    "                                       sin_denominator=False, cos_denominator=False, AFM_LJ_pos_degree=False, AFM_LJ_pos_degree_degrees = None, AFM_LJ=False,\n",
    "                                       AFM_LJ_damp=False, AFM_LJ_damp_degrees_e1 = None, AFM_LJ_damp_degrees_e2 = None,\n",
    "                                       one_over_eta =False, one_over_eta_degrees = None, \n",
    "                                       DMT_rep = False, DMT_att = False, DMT_rep_smooth=False, DMT_smooth_viscoel_damp = False, \n",
    "                                       DMT_viscoel_damp=False, DMT_visc_elast_degrees_e1=None, DMT_visc_elast_degrees_e2=None):\n",
    "    \n",
    "    #Version 2.0: This function uses candidate functions created with lambda functions to generate a library of candidate functions for SINDy.\n",
    "\n",
    "    #Parameters: DOF: Number of generalized coordinates in the system. (DOF=3 if we have x,y,phase)\n",
    "    #            poly: Boolean to determine if normal polynomial terms should be included in the library. \n",
    "    #            poly_degrees: A list that states the degrees that the library should create for normal polynomial functions. Example: poly_degrees = [2, (4,6), 8] (generates 2nd, 4th,5th, 6th and 8th degree polynomials)\n",
    "    #            special: Boolean to determine if a special AFM function should be created. Special AFM are of the form \"(1-x)^(-degree)\"\n",
    "    #            special_degrees: A list that states the degrees that the library should create for special AFM polynomial functions. Example: special_degrees = [2, (4,6), 8] (generates 2nd, 4th,5th, 6th and 8th degree special polynomials)\n",
    "    #            sin: Boolean to determine if a np.sin() should be created \n",
    "    #            cos: Boolean to determine if a np.cos() should be created\n",
    "    #            sin_denominator: Boolean to determine if a \"np.sin()^(-degree)\"\" should be created \n",
    "    #            cos_denominator: Boolean to determine if a \"np.cos()^(-degree)\"\" should be created \n",
    "    #            n_frequencies: A list that states the frequencies that the library should create for sine, cosine functions. Example: n_frequencies = [2, (4,6), 8] (sin(2*x), sin(4*x), sin(5*x), sin(6*x), sin(8*x))\n",
    "    #            AFM_amp: Is the y_bar value of an AFM equation. It is an integer value (a number).\n",
    "    #            poly_frac: Boolean to determine if a rational polynomial \"x^(-degree)\" should be created.\n",
    "    #            AFM_z: Boolean to fetermine if a AFM term with the shape of \"1-x-y_bar*np.sin(t)\" is created. \n",
    "\n",
    "\n",
    "    library_functions = []\n",
    "    library_function_names = []\n",
    "\n",
    "    if poly:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in poly_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            poly_func = make_poly(degree)\n",
    "            poly_name = make_poly_name(degree)\n",
    "            library_functions.append(poly_func)\n",
    "            library_function_names.append(poly_name)\n",
    "\n",
    "    if poly_frac:\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            poly_frac_func = make_poly_frac(degree)\n",
    "            poly_frac_name = make_poly_frac_name(degree)\n",
    "            library_functions.append(poly_frac_func)\n",
    "            library_function_names.append(poly_frac_name)\n",
    "\n",
    "    frequencies_to_generate = set()\n",
    "    for frequency_spec in n_frequencies:\n",
    "        frequencies_to_generate.update(to_ranges(frequency_spec))\n",
    "\n",
    "    if sin:\n",
    "        for frequency in sorted(frequencies_to_generate):\n",
    "            func = make_sine(frequency)\n",
    "            func_name = make_sine_name(frequency)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "            \n",
    "            if sin_denominator:\n",
    "                for degree in sorted(degrees_to_generate):\n",
    "                    func_denom = make_denom_sine(frequency, degree)\n",
    "                    func_name_denom = make_denom_sine_name(frequency, degree)\n",
    "                    library_functions.append(func_denom)\n",
    "                    library_function_names.append(func_name_denom)\n",
    "\n",
    "    if cos:\n",
    "        for frequency in sorted(frequencies_to_generate):\n",
    "            func = make_cosine(frequency)\n",
    "            func_name = make_cosine_name(frequency)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "            \n",
    "            if cos_denominator:\n",
    "                for degree in sorted(degrees_to_generate):\n",
    "                    func_denom = make_denom_cosine(frequency, degree)\n",
    "                    func_name_denom = make_denom_cosine_name(frequency, degree)\n",
    "                    library_functions.append(func_denom)\n",
    "                    library_function_names.append(func_name_denom)\n",
    "    if AFM_LJ:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in AFM_LJ_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_AFM_LJ_func(degree)\n",
    "            func_name = make_AFM_LJ_func_name(degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    if AFM_LJ_pos_degree:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in AFM_LJ_pos_degree_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_AFM_LJ_pos_degree_func(degree=degree)\n",
    "            func_name = make_AFM_LJ_pos_degree_name(degree=degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    if AFM_LJ_damp:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in AFM_LJ_damp_degrees_e1:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "            \n",
    "        degrees_to_generate_for_y = set()\n",
    "        for degree_spec_for_y in AFM_LJ_damp_degrees_e2:\n",
    "            degrees_to_generate_for_y.update(to_ranges(degree_spec_for_y))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            for degree_2 in sorted(degrees_to_generate_for_y):\n",
    "                DMT_visco_damp = make_AFM_LJ_damp_func(degree=degree, degree_2=degree_2)\n",
    "                DMT_visco_damp_name = make_AFM_LJ_damp_name(degree=degree, degree_2=degree_2)\n",
    "                library_functions.append(DMT_visco_damp)\n",
    "                library_function_names.append(DMT_visco_damp_name)\n",
    "\n",
    "    if DMT_att:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in DMT_att_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_AFM_DMT_att_func(x=DMT_data, a0=a0_val, degree=degree)\n",
    "            func_name = make_AFM_DMT_att_name(a0=a0_val, degree=degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    if DMT_rep:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in DMT_rep_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_AFM_DMT_rep_func(x=DMT_data, a0=a0_val, degree=degree)\n",
    "            func_name = make_AFM_DMT_rep_name(a0=a0_val, degree=degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    if one_over_eta:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in one_over_eta_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_1_over_eta_func(degree=degree)\n",
    "            func_name = make_1_over_eta_func_name(degree=degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    if DMT_viscoel_damp:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in DMT_visc_elast_degrees_e1:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "            \n",
    "        degrees_to_generate_for_y = set()\n",
    "        for degree_spec_for_y in DMT_visc_elast_degrees_e2:\n",
    "            degrees_to_generate_for_y.update(to_ranges(degree_spec_for_y))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            for degree_2 in sorted(degrees_to_generate_for_y):\n",
    "                DMT_visco_damp = make_AFM_DMT_viscoel_damp_func(x=DMT_data, y=DMT_data, a0=a0_val, degree=degree, degree_2=degree_2)\n",
    "                DMT_visco_damp_name = make_AFM_DMT_viscoel_damp_name(a0=a0_val, degree=degree, degree_2=degree_2)\n",
    "                library_functions.append(DMT_visco_damp)\n",
    "                library_function_names.append(DMT_visco_damp_name)\n",
    "\n",
    "    if exp_damp:\n",
    "        degrees_to_generate = set()\n",
    "        for degree_spec in exp_damp_degrees:\n",
    "            degrees_to_generate.update(to_ranges(degree_spec))\n",
    "\n",
    "        for degree in sorted(degrees_to_generate):\n",
    "            func = make_exp_damping_func(z_b=z_b_val, degree=degree)\n",
    "            func_name = make_exp_damping_name(z_b=z_b_val, degree=degree)\n",
    "            library_functions.append(func)\n",
    "            library_function_names.append(func_name)\n",
    "\n",
    "    return library_functions, library_function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded6799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_cand_function(SINDy_model, cand_function_to_check):\n",
    "    index = None\n",
    "    for idx, cand_func in enumerate(SINDy_model.get_feature_names()):\n",
    "        if cand_func == cand_function_to_check:\n",
    "            index= idx\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1a7a5",
   "metadata": {},
   "source": [
    "## Functions for Candidate Function Reduction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25264dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_feature_names(feature_string, name_mapping):\n",
    "    # Use regular expressions to replace all occurrences of variable names\n",
    "    for old_name, new_name in name_mapping.items():\n",
    "        # Double the backslashes in new_name for use in re.sub\n",
    "        safe_new_name = new_name.replace('\\\\', '\\\\\\\\')\n",
    "        feature_string = re.sub(r'\\b' + re.escape(old_name) + r'\\b', safe_new_name, feature_string)\n",
    "    return feature_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1b788",
   "metadata": {},
   "source": [
    "## Functions to generate Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddf2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cluster_centers(x_train_mult_trajectories, num_points_high_res, sub_sample_val, filter_vel_max_val, index_to_plot, centers_loc, multiple_init_cond = False, plot_file_name = None, plot=False, save_plot=False):\n",
    "\n",
    "    # Version 1.0: This function takes an array of arrays with multiple initial conditions and places points along the phase space that later would be used as\n",
    "    # cluster centers\n",
    "    #\n",
    "    # Parameters: x_train_mult_trajectories: List of arrays, where each array contain the time signal of an initial condition with shape [num_of_data_points, 3] \n",
    "    #             num_points_high_res: Integer defining the number of high-resolution points to be used when interpolating the trajectory in the phase space.\n",
    "    #             sub_sample_val: Integer defining the step size for subsampling the high-resolution points to reduce the total number of cluster centers.\n",
    "    #             filter_vel_max_val: Float defining the maximum velocity threshold, below which points are considered valid for being cluster centers.\n",
    "    #             index_to_plot: Integer index specifying which trajectory from the list should be plotted (used when `plot` is True).\n",
    "    #             centers_loc: Integer determining the location of centers; typically 2 for synthetic data and 5 or 6 for experimental data, affecting how the starting and ending points of closed orbits are calculated.\n",
    "    #             multiple_init_cond: Boolean indicating if centers are being defined for multiple initial conditions at the same time. Default is False.\n",
    "    #             plot_file_name: String specifying the filename for the plot, if saved. Relevant when `save_plot` is True.\n",
    "    #             plot: Boolean to control whether to plot the phase space with the defined centers. Default is False.\n",
    "    #             save_plot: Boolean to control whether to save the plot to a file. Default is False.\n",
    "\n",
    "    cluster_centers_x = []\n",
    "    cluster_centers_y = []\n",
    "\n",
    "    if multiple_init_cond:\n",
    "        for analyzed_trajectory in x_train_mult_trajectories:\n",
    "            init_closed_orbit = (analyzed_trajectory.shape[0]//centers_loc) - 1000  #centers_loc is usually 2 for synthetic data\n",
    "            end_closed_orbit = (analyzed_trajectory.shape[0]//centers_loc) + 1000   #centers_loc is usually 5 or 6 for experimental data\n",
    "\n",
    "            #To extract the last closed orbit\n",
    "            x = analyzed_trajectory[init_closed_orbit:end_closed_orbit,0] #[-650:,0]\n",
    "            v = analyzed_trajectory[init_closed_orbit:end_closed_orbit,1]\n",
    "\n",
    "            #Calculate the arc length for parameterization\n",
    "            arc_lengths = np.zeros_like(x)\n",
    "            arc_lengths[1:] = np.cumsum(np.sqrt(np.diff(x)**2 + np.diff(v)**2))\n",
    "\n",
    "            #Parameterize the orbit by arc length\n",
    "            spline_x = CubicSpline(arc_lengths, x)\n",
    "            spline_v = CubicSpline(arc_lengths, v)\n",
    "\n",
    "            # Initial high-resolution sampling\n",
    "            high_res_lengths = np.linspace(arc_lengths[0], arc_lengths[-1], num_points_high_res)\n",
    "            high_res_x = spline_x(high_res_lengths)\n",
    "            high_res_v = spline_v(high_res_lengths)\n",
    "\n",
    "            # Subsample every nth point, n determines the new spacing\n",
    "            n = sub_sample_val \n",
    "            subsampled_x = high_res_x[::n]\n",
    "            subsampled_v = high_res_v[::n]\n",
    "\n",
    "            # Filter to keep only points with negative velocity\n",
    "            negative_velocity_mask = subsampled_v < filter_vel_max_val\n",
    "            filtered_x = subsampled_x[negative_velocity_mask]\n",
    "            filtered_v = subsampled_v[negative_velocity_mask]\n",
    "            \n",
    "            cluster_centers_x.append(filtered_x)\n",
    "            cluster_centers_y.append(filtered_v)\n",
    "\n",
    "        if plot:\n",
    "            fig, axs = plt.subplots(1, 1, figsize=(10, 8)) #[-650:,0]\n",
    "            axs.plot(x_train_mult_trajectories[index_to_plot][:,0], x_train_mult_trajectories[index_to_plot][:,1], color='black', label='Last closed orbit', alpha = 0.7)\n",
    "            axs.scatter(cluster_centers_x[index_to_plot], cluster_centers_y[index_to_plot], color='blue', label='Cluster Centers')\n",
    "            axs.set_title('Phase Space Plot with Cluster Center \\n Points Having Negative Velocity', fontsize=20)\n",
    "            axs.set_xlabel(r'Displacement $\\bar{\\eta}_1$ [-]', fontsize=20)\n",
    "            axs.set_ylabel(r' Velocity $\\bar{\\eta}_2$ [-]', fontsize=20)\n",
    "            axs.legend(loc = 'upper left')\n",
    "            axs.tick_params(axis='both', which='major', labelsize=20)  # Customize the font size of the tick labels\n",
    "            axs.tick_params(axis='both', which='major', labelsize=20)  # Customize the font size of the tick label\n",
    "\n",
    "            if save_plot:\n",
    "                fig.savefig(f'{plot_file_name}.png', transparent=True, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        analyzed_trajectory = x_train_mult_trajectories\n",
    "        init_closed_orbit = (analyzed_trajectory.shape[0]//centers_loc) - 1000  #centers_loc is usually 2 for synthetic data\n",
    "        end_closed_orbit = (analyzed_trajectory.shape[0]//centers_loc) + 1000   #centers_loc is usually 5 or 6 for experimental data\n",
    "\n",
    "        x = analyzed_trajectory[init_closed_orbit:end_closed_orbit,0]\n",
    "        v = analyzed_trajectory[init_closed_orbit:end_closed_orbit,1]  #[50000:53000]\n",
    "\n",
    "        #Calculate the arc length for parameterization\n",
    "        arc_lengths = np.zeros_like(x)\n",
    "        arc_lengths[1:] = np.cumsum(np.sqrt(np.diff(x)**2 + np.diff(v)**2))\n",
    "\n",
    "        #Parameterize the orbit by arc length\n",
    "        spline_x = CubicSpline(arc_lengths, x)\n",
    "        spline_v = CubicSpline(arc_lengths, v)\n",
    "\n",
    "        # Initial high-resolution sampling\n",
    "        high_res_lengths = np.linspace(arc_lengths[0], arc_lengths[-1], num_points_high_res)\n",
    "        high_res_x = spline_x(high_res_lengths)\n",
    "        high_res_v = spline_v(high_res_lengths)\n",
    "\n",
    "        # Subsample every nth point, n determines the new spacing\n",
    "        n = sub_sample_val \n",
    "        subsampled_x = high_res_x[::n]\n",
    "        subsampled_v = high_res_v[::n]\n",
    "\n",
    "        # Filter to keep only points with negative velocity\n",
    "        negative_velocity_mask = subsampled_v < filter_vel_max_val\n",
    "        filtered_x = subsampled_x[negative_velocity_mask]\n",
    "        filtered_v = subsampled_v[negative_velocity_mask]\n",
    "        \n",
    "        cluster_centers_x.append(filtered_x)\n",
    "        cluster_centers_y.append(filtered_v)\n",
    "\n",
    "        if plot:\n",
    "            fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "            axs.plot(x_train_mult_trajectories[:,0], x_train_mult_trajectories[:,1], color='black', label='HOPG Expeimental Data', linewidth=0.5, alpha=0.7)\n",
    "            axs.scatter(cluster_centers_x[0], cluster_centers_y[0], color='blue', label='Cluster Centers')\n",
    "            axs.set_title('Phase Space Plot with Cluster Center \\n Points Having Negative Velocity', fontsize=20)\n",
    "            axs.set_xlabel(r'Displacement $\\bar{\\eta}_1$ [-]', fontsize=20)\n",
    "            axs.set_ylabel(r' Velocity $\\bar{\\eta}_2$ [-]', fontsize=20)\n",
    "            axs.legend(loc = 'upper left')\n",
    "            axs.tick_params(axis='both', which='major', labelsize=20)  # Customize the font size of the tick labels\n",
    "            axs.tick_params(axis='both', which='major', labelsize=20)  # Customize the font size of the tick label\n",
    "\n",
    "            if save_plot:\n",
    "                fig.savefig(f'{plot_file_name}.png', transparent=True, dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "    return cluster_centers_x, cluster_centers_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5758f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_clusters_from_centers(cluster_size, x_train_mult_trajectories, KNN_neighbors_num, mult_traj_cluster_centers_x, mult_traj_cluster_centers_y, KNN_radius = 1.0, multiple_init_cond=False):\n",
    "\n",
    "    # Version 1.0: This function works by fitting a nearest neighbors classifier to the trajectory points and using it to find the closest neighbors around predefined\n",
    "    #              cluster centers specified by the mult_traj_cluster_centers_x and mult_traj_cluster_centers_y parameters. Clusters are defined by including a fixed \n",
    "    #              number of points around each center, dictated by the cluster_size parameter. If multiple_init_cond is set to True, the function processes multiple trajectories independently.\n",
    "    # \n",
    "    # Parameters:  cluster_size: Integer specifying the number of data points to include in each cluster around the cluster center.\n",
    "    #              x_train_mult_trajectories: List of arrays, where each array is a trajectory with shape [num_of_data_points, dimensions] typically dimensions being 3 for 3D data.\n",
    "    #              KNN_neighbors_num: Integer specifying the number of nearest neighbors to consider for determining the cluster membership.\n",
    "    #              mult_traj_cluster_centers_x: List of lists containing the x-coordinates of cluster centers for multiple trajectories.\n",
    "    #              mult_traj_cluster_centers_y: List of lists containing the y-coordinates of cluster centers for multiple trajectories.\n",
    "    #              KNN_radius: Float specifying the radius within which to search for nearest neighbors. Default is 1.0.\n",
    "    #              multiple_init_cond: Boolean indicating if the function should handle multiple trajectories simultaneously. Default is False.\n",
    "\n",
    "    #Returns:\n",
    "    #    mult_traj_clusters_sections: List of lists, where each sublist contains arrays of sections of trajectories that are part of a cluster.\n",
    "    #    mult_traj_clusters_dots_list: List of lists, where each sublist contains the indices of the points in the trajectory that form a cluster around a center.\n",
    "\n",
    "    mult_traj_clusters_dots_list = []\n",
    "    mult_traj_clusters_sections = []\n",
    "\n",
    "    if multiple_init_cond:\n",
    "        for i, analyzed_trajectory in enumerate(x_train_mult_trajectories):\n",
    "            KNN_traj_for_fit_temp = np.vstack((analyzed_trajectory[:,0], analyzed_trajectory[:,1])).T\n",
    "            neigh = NearestNeighbors(n_neighbors=KNN_neighbors_num, radius = KNN_radius)\n",
    "            neigh.fit(KNN_traj_for_fit_temp)\n",
    "\n",
    "            single_traj_cluster_dots_list = []\n",
    "            for index_center in range(len(mult_traj_cluster_centers_x[i])):\n",
    "                cluster_dots_temp = neigh.kneighbors([[mult_traj_cluster_centers_x[i][index_center], mult_traj_cluster_centers_y[i][index_center]]], return_distance=False)\n",
    "                single_traj_cluster_dots_list.append(cluster_dots_temp[0]) #this is always index [0] because neigh.kneighbors() gives a (1,100) array by default. So this gives (100,) already\n",
    "            mult_traj_clusters_dots_list.append(single_traj_cluster_dots_list)\n",
    "            \n",
    "        for j, single_traj_clusters_dots in enumerate(mult_traj_clusters_dots_list):\n",
    "            analyzed_trajectory = x_train_mult_trajectories[j]\n",
    "\n",
    "            single_traj_clusters_sections = []\n",
    "            for single_cluster_dots in single_traj_clusters_dots:\n",
    "                single_traj_single_cluster_sections = []\n",
    "                for single_cluster_dot in single_cluster_dots:\n",
    "                    single_trajectory_section = np.vstack((analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),0], analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),1], analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),2])).T\n",
    "                    single_traj_single_cluster_sections.append(single_trajectory_section)\n",
    "                single_traj_clusters_sections.append(single_traj_single_cluster_sections)\n",
    "            mult_traj_clusters_sections.append(single_traj_clusters_sections) \n",
    "\n",
    "    else:\n",
    "        analyzed_trajectory = x_train_mult_trajectories\n",
    "        KNN_traj_for_fit_temp = np.vstack((analyzed_trajectory[:,0], analyzed_trajectory[:,1])).T\n",
    "        neigh = NearestNeighbors(n_neighbors=KNN_neighbors_num, radius=KNN_radius)\n",
    "        neigh.fit(KNN_traj_for_fit_temp)\n",
    "\n",
    "        single_traj_cluster_dots_list = []\n",
    "        for index_center in range(len(mult_traj_cluster_centers_x)):\n",
    "            cluster_dots_temp = neigh.kneighbors([[mult_traj_cluster_centers_x[index_center], mult_traj_cluster_centers_y[index_center]]], return_distance=False)\n",
    "            single_traj_cluster_dots_list.append(cluster_dots_temp[0]) #this is always index [0] because neigh.kneighbors() gives a (1,100) array by default. So this gives (100,) already\n",
    "        mult_traj_clusters_dots_list.append(single_traj_cluster_dots_list)\n",
    "\n",
    "        single_traj_clusters_dots = mult_traj_clusters_dots_list[0]\n",
    "        single_traj_clusters_sections = []\n",
    "        for single_cluster_dots in single_traj_clusters_dots:\n",
    "            single_traj_single_cluster_sections = []\n",
    "            for single_cluster_dot in single_cluster_dots:\n",
    "                single_trajectory_section = np.vstack((analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),0], analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),1], analyzed_trajectory[(single_cluster_dot-cluster_size):(single_cluster_dot+cluster_size),2])).T\n",
    "                single_traj_single_cluster_sections.append(single_trajectory_section)\n",
    "            single_traj_clusters_sections.append(single_traj_single_cluster_sections)\n",
    "        mult_traj_clusters_sections.append(single_traj_clusters_sections) \n",
    "\n",
    "    return mult_traj_clusters_sections, mult_traj_clusters_dots_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa39409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_close_points(x_points, y_points, threshold=0.2):\n",
    "    filtered_x = []\n",
    "    filtered_y = []\n",
    "\n",
    "    for x, y in zip(x_points, y_points):\n",
    "        # Calculate distances from the current point to all points in the filtered lists\n",
    "        if filtered_x: \n",
    "            distances = np.sqrt((np.array(filtered_x) - x) ** 2 + (np.array(filtered_y) - y) ** 2)\n",
    "            if np.all(distances > threshold):\n",
    "                filtered_x.append(x)\n",
    "                filtered_y.append(y)\n",
    "        else:\n",
    "            filtered_x.append(x)\n",
    "            filtered_y.append(y)\n",
    "\n",
    "    return np.array(filtered_x), np.array(filtered_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc806b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_centroids_from_data(analyzed_trajectories, neighbors_num, normalize_data=False):\n",
    "\n",
    "    x = analyzed_trajectories[:, 0]        # All displacement values\n",
    "    y = analyzed_trajectories[:, 1]        # All velocity values\n",
    "    times = analyzed_trajectories[:, 2]    # All time values\n",
    "\n",
    "    if normalize_data:\n",
    "        # Normalize the data\n",
    "        x_data = x / max(x)\n",
    "        y_data = y / max(y)\n",
    "    else:\n",
    "        x_data = x \n",
    "        y_data = y\n",
    "\n",
    "    # Combine into a single array of coordinates\n",
    "    coordinates = np.vstack((x_data, y_data)).T\n",
    "\n",
    "    # Initialize NearestNeighbors with the number of neighbors 'neighbors_num'\n",
    "    nn = NearestNeighbors(n_neighbors=neighbors_num, algorithm='auto').fit(coordinates)\n",
    "    # Find the k-neighbors of each point\n",
    "    distances, indices = nn.kneighbors(coordinates)\n",
    "\n",
    "    # Calculate cluster_centroid\n",
    "    cluster_centroids = []\n",
    "    for idx in indices:\n",
    "        cluster_points = coordinates[idx]\n",
    "        centroid = cluster_points.mean(axis=0)\n",
    "        cluster_centroids.append(centroid)\n",
    "\n",
    "    cluster_centroids = np.array(cluster_centroids)\n",
    "\n",
    "    return cluster_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3e493",
   "metadata": {},
   "source": [
    "## Functions for training SINDy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serial_number(index, cluster, lambda_value, nu_value):\n",
    "   # Version 1.0: This function generates a unique serial number for when a trained SINDy model is saved.\n",
    "\n",
    "   # Parameters:  index: Integer specifying the number of data points to include in each cluster around the cluster center.\n",
    "   #              cluster: number of the cluster being analyzed\n",
    "   #              lambda_value: lambda value used for the ContrainedSR3() optimizer\n",
    "   #              nu_value: nu value used for the ContrainedSR3() optimizer\n",
    "\n",
    "    date_str = datetime.now().strftime(\"%d-%m-%y\")  # Current date in dd-mm-yy format\n",
    "    unique_number = f\"{index:03d}\"  # Zero-padded 3-digit number\n",
    "    return f\"{date_str}-{unique_number}-{cluster}_cluster_{cluster}_lambda_{lambda_value}_nu_{nu_value}_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_constrained_SINDy_model(candidate_func_library, model_feature_names, constraint_rhs_array, constraint_lhs_array, trajectories_data, lambda_val, traject_dt, nu_val, ensemble = False, multiple_trajectories = False, model_filename = None, save_model = False):\n",
    "    # Version 1: This function is used to fit/train a SINDy model using ConstrainedSINDy with ConstrainedSR3. \n",
    "    #\n",
    "    # Parameters: candidate_func_library: Number of lambda values to generate\n",
    "    #             model_feature_names: smallest order of magnitude for generated lambdas\n",
    "    #             constraint_rhs_array: numpy array containing the right hand side constraints for ContrainedSR3()\n",
    "    #             constraint_lhs_array: numpy array containing the left hand side constraints for ContrainedSR3()\n",
    "    #             trajectories_data: The time-series data from the system to be analyzed. \n",
    "    #             lambda_val: lambda value used for the ContrainedSR3() optimizer\n",
    "    #             nu_val: nu value used for the ContrainedSR3() optimizer\n",
    "    #             traject_dt: time step of the time-series data.\n",
    "    #             model_filename: Name for the .dill file used to save the trained model if needed.\n",
    "\n",
    "    model_optimizer = ps.ConstrainedSR3(constraint_rhs=constraint_rhs_array, constraint_lhs=constraint_lhs_array, threshold=lambda_val, nu = nu_val, tol=1e-9, thresholder=\"l0\", max_iter=10000)\n",
    "    constrained_model = ps.SINDy(optimizer=model_optimizer, feature_library=candidate_func_library, feature_names=model_feature_names)\n",
    "    constrained_model.fit(trajectories_data, t=traject_dt, ensemble=ensemble, multiple_trajectories = multiple_trajectories, quiet=True)\n",
    "\n",
    "    if save_model:\n",
    "        with open(model_filename + '.dill', 'wb') as f: #saves the model with the .dill extension within the str name, for clarity on how to open the models later\n",
    "            dill.dump(constrained_model, f)\n",
    "    \n",
    "    return constrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_lambda_n_nu(num_values, smaller_order_magnitude, bigger_order_magnitude, randomize=False):\n",
    "    # Version 1: This function generates an array of lambda coeffieints to be used as threshold values for a \n",
    "    #            SINDy algorithm. I genereates lambda values from a small order to a big order of magnitude\n",
    "    #            by creating logaritmically spaces values. \n",
    "    # Parameters: num_values: Number of lambda values to generate\n",
    "    #             smaller_order_magnitude: smallest order of magnitude for generated lambdas\n",
    "    #             bigger_order_magnitude: biggest order of magnitude for generated lambdas\n",
    "    #             randomize: Boolean than determines if the order of lambdas is randomized in the final list.\n",
    "\n",
    "    # Generate logarithmically spaced values\n",
    "    log_values = np.linspace(smaller_order_magnitude, bigger_order_magnitude, num_values)\n",
    "    values = 10**log_values\n",
    "\n",
    "    if randomize:\n",
    "        np.random.shuffle(values)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27e78f",
   "metadata": {},
   "source": [
    "## Functions for opening trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trained_models_from_folder(folder_path, folder_name):\n",
    "    # Version 1: This function imports all saved SINDy models that are within a folder \n",
    "    # Parameters: folder_path: Path where the folder is located\n",
    "    #             folder_name: Name of the folder with the models to upload\n",
    "\n",
    "    # Define the path to the folder\n",
    "    path = f'{folder_path}/{folder_name}/*.dill'\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    # Initialize lists and dictionary\n",
    "    files_list = []  # List of file names\n",
    "    loaded_models_list = []  # List of loaded models\n",
    "    models_dict = {}  # Dictionary of models with file names as keys\n",
    "\n",
    "    # Iterate through the files\n",
    "    for file in files:\n",
    "        # Extract the file name\n",
    "        file_name = file.replace(f'{folder_path}/{folder_name}/', \"\")\n",
    "        files_list.append(file_name)\n",
    "\n",
    "        # Load the model\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_model = dill.load(f)\n",
    "        loaded_models_list.append(loaded_model)\n",
    "\n",
    "        # Add to the dictionary\n",
    "        models_dict[file_name] = loaded_model\n",
    "\n",
    "    # Return all three structures\n",
    "    return files_list, loaded_models_list, models_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28292674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_single_trained_model_from_folder(folder_path, folder_name, file_name):\n",
    "#   Version 1: This function imports a single SINDy trained model that has been saved in a .dill format\n",
    "#\n",
    "#   Parameters: folder_name: path with the name of the folder where the model to be uploaded is located\n",
    "#               file_name: the name of the model needed to be uploaded\n",
    "    # Define the full path to the file\n",
    "    path = f'{folder_path}/{folder_name}/{file_name}'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No file found at {path}\")\n",
    "\n",
    "    # Load the model\n",
    "    with open(path, 'rb') as f:\n",
    "        loaded_model = dill.load(f)\n",
    "    \n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60878e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_trained_models_from_folder_to_dataframe(folder_path, folder_name, new_dataframe, number_of_parsimony_eq):\n",
    "#    Version 1: This function imports all saved SINDy models that are within a folder and places them in a Pandas dataframe\n",
    "#    Parameters: folder_path: Path where the folder is located\n",
    "#                folder_name: Name of the folder with the models to upload\n",
    "\n",
    "    # Define the path to the folder\n",
    "    path = f'{folder_path}/{folder_name}/*.dill'\n",
    "    files = glob.glob(path)\n",
    "\n",
    "    # Initialize lists and dictionary\n",
    "    files_list = []  # List of file names\n",
    "    loaded_models_list = []  # List of loaded models\n",
    "    models_dict = {}  # Dictionary of models with file names as keys\n",
    "\n",
    "    # Iterate through the files\n",
    "    for file in files:\n",
    "        # Extract the file name\n",
    "        file_name = file.replace(f'{folder_path}/{folder_name}/', \"\")\n",
    "        files_list.append(file_name)\n",
    "\n",
    "        # Load the model\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_model = dill.load(f)\n",
    "        loaded_models_list.append(loaded_model)\n",
    "\n",
    "        headings_from_file = file_name.split('_')\n",
    "\n",
    "        #Checking parsimony:\n",
    "        if number_of_parsimony_eq == 1: \n",
    "            eq_fund_in_table = convert_to_table(loaded_model.equations(precision=9)[0])\n",
    "            fund_eq_len = eq_fund_in_table.count()\n",
    "\n",
    "        if number_of_parsimony_eq == 2: \n",
    "            eq_fund_in_table = convert_to_table(loaded_model.equations(precision=9)[1])\n",
    "            fund_eq_len = eq_fund_in_table.count()\n",
    "\n",
    "        # Add to the dictionary\n",
    "        models_dict[file_name] = loaded_model\n",
    "        new_row = {new_dataframe.columns[0]:[loaded_model],\n",
    "                    new_dataframe.columns[1]:[headings_from_file[2].capitalize()],\n",
    "                    new_dataframe.columns[2]:[headings_from_file[4]],\n",
    "                    new_dataframe.columns[3]:[headings_from_file[6]],\n",
    "                    new_dataframe.columns[4]:[headings_from_file[8]],\n",
    "                    new_dataframe.columns[5]:[headings_from_file[10]],\n",
    "                    new_dataframe.columns[6]:[fund_eq_len]} \n",
    "        new_dataframe = pd.concat([new_dataframe, pd.DataFrame(new_row)], ignore_index=True)\n",
    "\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_unique_models_in_dataframe(df):\n",
    "#     Version 1: This function marks models in the DataFrame as unique or not based on their coefficients and active terms.\n",
    "#     Parameters: df (pd.DataFrame): DataFrame containing trained models.\n",
    "#                 folder_name: Name of the folder with the models to upload\n",
    "#.    Returns: pd.DataFrame: Updated DataFrame with an additional 'Is Unique' column.\n",
    "\n",
    "    encountered_models = set()\n",
    "    uniqueness_flags = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        model = row['Candidate Model']  # Extract the model\n",
    "\n",
    "        # Extract coefficients and candidate functions\n",
    "        coefficients = np.round(model.coefficients(), decimals=7)  # Normalize coefficients\n",
    "        terms = model.get_feature_names()  # Full library\n",
    "\n",
    "        # Extract only active terms\n",
    "        active_terms = [\n",
    "            terms[i] for i, coef in enumerate(coefficients) if not np.allclose(coef, 0)\n",
    "        ]\n",
    "        active_coefficients = [\n",
    "            tuple(coef) for coef in coefficients if not np.allclose(coef, 0)\n",
    "        ]\n",
    "\n",
    "        # Create a unique signature for the model\n",
    "        model_signature = (tuple(active_coefficients), tuple(active_terms))\n",
    "\n",
    "        # Check for uniqueness\n",
    "        if model_signature not in encountered_models:\n",
    "            encountered_models.add(model_signature)\n",
    "            uniqueness_flags.append(True)  # Mark as unique\n",
    "        else:\n",
    "            uniqueness_flags.append(False)  # Mark as not unique\n",
    "\n",
    "    # Add the uniqueness column to the DataFrame\n",
    "    df['Is Unique'] = uniqueness_flags\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e95ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_trained_models_from_dataframe(dataframe_with_models):\n",
    "\n",
    "    unique_models_dict = {}  \n",
    "    unique_models_list = []\n",
    "    unique_model_names = []\n",
    "    encountered_models = set()\n",
    "\n",
    "    for index, loaded_model in enumerate(dataframe_with_models['Candidate Model']):\n",
    "\n",
    "        # Extract coefficients and candidate functions\n",
    "        coefficients = np.round(loaded_model.coefficients(), decimals=7)  # Normalize coefficients\n",
    "        terms = loaded_model.get_feature_names()  # Full library\n",
    "\n",
    "        # Extract only active terms\n",
    "        active_terms = [\n",
    "            terms[i] for i, coef in enumerate(coefficients) if not np.allclose(coef, 0)\n",
    "        ]\n",
    "        active_coefficients = [\n",
    "            tuple(coef) for coef in coefficients if not np.allclose(coef, 0)\n",
    "        ]\n",
    "\n",
    "        # Create a unique signature for the model\n",
    "        model_signature = (tuple(active_coefficients), tuple(active_terms))\n",
    "\n",
    "        model_serial_number = dataframe_with_models.iloc[index]['Serial No.']\n",
    "\n",
    "        # Check for uniqueness\n",
    "        if model_signature not in encountered_models:\n",
    "            encountered_models.add(model_signature)\n",
    "            unique_models_dict[model_serial_number] = loaded_model  \n",
    "            unique_models_list.append(loaded_model)\n",
    "            unique_model_names.append(model_serial_number)\n",
    "\n",
    "    return unique_model_names, unique_models_list, unique_models_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614b9ab",
   "metadata": {},
   "source": [
    "## Functions for Simulating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fdc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_F_ts_from_found_model_DMT(found_model, simulated_model):\n",
    "#   Version 1: This function receives a SINDy model that has been fit before. \n",
    "#\n",
    "#   Parameters: found_model: This is a SINDy that has been previously trained. \n",
    "#               simulated_model: The found SINDy model that has been simulated with model.simulate()\n",
    "    \n",
    "    model_copy = copy.deepcopy(found_model) # Create a deep copy of the original model to avoid modifying it\n",
    "\n",
    "    coefficients = model_copy.coefficients()\n",
    "    library_terms = model_copy.get_feature_names()\n",
    "\n",
    "    # Modify the coefficients of the copied model\n",
    "    for i, term in enumerate(library_terms):\n",
    "        if term == 'e1':  # Removes stiffness terms out of the e2' equation only. \n",
    "            coefficients[1, i] = 0 \n",
    "        elif term == 'e2': # Removes damping terms out of the e2' equation only. \n",
    "            coefficients[1, i] = 0\n",
    "        elif term == 'sin(1*phase)': # Removes Forcing terms out of the e2' equation only. \n",
    "            coefficients[1, i] = 0\n",
    "        elif term == '1': # Removes constant terms out of the e2' equation only. \n",
    "            coefficients[1, i] = 0\n",
    "\n",
    "    model_copy.coefficients_ = coefficients\n",
    "\n",
    "    force_list = []\n",
    "    simulated_F_ts = []\n",
    "\n",
    "    # Simulate using the modified copy\n",
    "    for current_point in simulated_model:\n",
    "        model_sim_eval_temp = model_copy.predict(current_point[np.newaxis, :])[0]\n",
    "        force_list.append(model_sim_eval_temp[0])\n",
    "        simulated_F_ts.append(-model_sim_eval_temp[1])\n",
    "\n",
    "    return simulated_F_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b687221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_F_ts_DMT_found(sim_disp_data, a0):\n",
    "    #Version 1.0: This function takes the array of simulated data and calculates the Tip-Sample Force using the DMT model where the found equations were written manually\n",
    "    #             from the found equations by AFM-SINDy\n",
    "\n",
    "    #Parameters: - sim_disp_data: Synthetic data from a found DMT model.\n",
    "    #              It can be either a single measurement in an np.array or multiple trajectories in a list.\n",
    "    #            - a0: found intermolecular distance.\n",
    "    if isinstance(sim_disp_data, np.ndarray):\n",
    "        F_ts_true_array = np.zeros_like(sim_disp_data[:,0],dtype=float)\n",
    "        for i in range(len(sim_disp_data)):\n",
    "            x_elem = sim_disp_data[i, 0]\n",
    "            y_elem = sim_disp_data[i, 1]\n",
    "            F_ts_temp = calculate_AFM_DMT_Force_found(eta_1=x_elem, eta_2 = y_elem, a0=a0)\n",
    "            F_ts_true_array[i] = F_ts_temp\n",
    "        return -1*F_ts_true_array \n",
    "\n",
    "    if isinstance(sim_disp_data, list):\n",
    "        F_ts_true_list = []\n",
    "        F_ts_true_array = np.zeros_like(sim_disp_data[0][:,0],dtype=float)\n",
    "        for trajectory in sim_disp_data:\n",
    "            for i in range(len(trajectory)):\n",
    "                x_elem = trajectory[i, 0]\n",
    "                y_elem = trajectory[i, 1]\n",
    "                F_ts_temp = calculate_AFM_DMT_Force_found(eta_1=x_elem, eta_2 = y_elem, a0=a0)\n",
    "                F_ts_true_array[i] = F_ts_temp\n",
    "            F_ts_true_list.append(F_ts_true_array) \n",
    "\n",
    "        return -1*F_ts_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_results_from_cluster(model_to_simulate, validation_trajectory_section, full_validation_trajectory, solve_ivp_method, dt, t_steps_beyond_cluster, a0, model_type, C1=None, C2=None, C3=None, disp_plot_file_name= None, vel_plot_file_name = None, force_plot_file_name= None, plot = False, save=False):\n",
    "#   Version 1: This function takes a model that has been trained and simulates it in time from the cluster from which it was found. \n",
    "# \n",
    "#   Parameters: model_to_simulate: Is the fit SINDy found model to simulate.\n",
    "#               validation_trajectory_section: Is the array single trajectory section within \"full_validation_trajectory\" that we can to use as validation for the simulated SINDy found model.\n",
    "#                                              Typically is a section of a cluster. Therefore, it must have a shape of (num_of_data, 3). (dist, vel, phase). Example: mult_traj_clusters_sections[traject][cluster_att][10]\n",
    "#               full_validation_trajectory: Is the complete trajectory starting from transient to steady state that can be used as validation. It is used to simulate a bit furhter than the cluster size. Example: x_train_DMT_mult_traj[traject]\n",
    "#               solve_ivp_method: Is a string to choose the method to perform the numerical integration like 'RK45' or 'BDF'.\n",
    "#               disp_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "#               force_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "\n",
    "    t_simulate = np.arange(validation_trajectory_section[0][2], validation_trajectory_section[0][2]+(t_steps_beyond_cluster*dt), dt) #This is the reason why it only works for attractive clusters so far. t_simulate can not go further than att region \"+2.5\"\n",
    "    init_cond = validation_trajectory_section[0]\n",
    "\n",
    "    simulated_model = model_to_simulate.simulate(init_cond, t_simulate, integrator_kws={\"method\": solve_ivp_method, \"rtol\": 1e-5, \"atol\": 1e-7})  #\"rtol\": 1e-5, \"atol\": 1e-7})\n",
    "\n",
    "    init_cond_index = np.where(full_validation_trajectory[:,2] == t_simulate[0])[0][0] #Find where in the full trajectory the t_simulate starts for plotting and giving a score\n",
    "    next_index = init_cond_index + simulated_model.shape[0] #Find where in the full trajectory the t_simulate ends or plotting and giving a score\n",
    "\n",
    "    extended_validation_traject = np.vstack((full_validation_trajectory[init_cond_index:next_index,0], full_validation_trajectory[init_cond_index:next_index,1], full_validation_trajectory[init_cond_index:next_index,2])).T\n",
    "\n",
    "    if model_type == 'Normal DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2)\n",
    "        F_ts_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "    elif model_type == 'Viscoelastic DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT_viscoelast_damp(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2, C3=C3)\n",
    "        F_ts_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "\n",
    "    if plot: \n",
    "        #Plots displacement:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(extended_validation_traject[:,2], extended_validation_traject[:,0], color = 'green', label = r'True $\\eta_1$')\n",
    "        axs.plot(t_simulate, simulated_model[:,0], '--', color = 'black', label = r'Found $\\eta_1$')\n",
    "        axs.plot(validation_trajectory_section[:,2], validation_trajectory_section[:,0], '-', color = 'red', linewidth = 3, alpha = 0.5, label = r'Cluster Extension')\n",
    "        axs.set_title('Comparison between true and found dynamics', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Displacement [-]', fontsize=25)\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "        axs.set_xlim([t_simulate[0], t_simulate[-1]])\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        if save:\n",
    "            plt.savefig(disp_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "        #Plots Velocity:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(extended_validation_traject[:,2], extended_validation_traject[:,1], color = 'green', label = r'True $\\eta_2$')\n",
    "        axs.plot(t_simulate, simulated_model[:,1], '--', color = 'black', label = r'Found $\\eta_2$')\n",
    "        axs.plot(validation_trajectory_section[:,2], validation_trajectory_section[:,1], '-', color = 'red', linewidth = 3, alpha = 0.5, label = r'Cluster Extension')\n",
    "        axs.set_title('Comparison between true and found dynamics', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Velocity [-]', fontsize=25)\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "        axs.set_xlim([t_simulate[0], t_simulate[-1]])\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        if save:\n",
    "            plt.savefig(vel_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "        #Checking parsimony:\n",
    "        eq_fund_tab_att_pareto = convert_to_table(model_to_simulate.equations(precision=9)[1])\n",
    "        eq_len_att_pareto = eq_fund_tab_att_pareto.count()\n",
    "\n",
    "        print('')\n",
    "        print('Equation with length: ' + str(eq_len_att_pareto))\n",
    "        model_to_simulate.print(precision=9)\n",
    "        print('')\n",
    "\n",
    "        #plot the force calculations:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(t_simulate, F_ts_true, color = 'green', label = r'True $F_{ts}$')\n",
    "        plt.plot(t_simulate, F_ts_sim, '--', color = 'black', label = r'Found $F_{ts}$')\n",
    "        plt.title('Comparison between true force and found force')\n",
    "        plt.xlabel('Time [-]')\n",
    "        plt.ylabel('Tip-Sample Force [-]')\n",
    "        plt.legend(loc='best')\n",
    "        if save:\n",
    "            plt.savefig(force_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    \n",
    "    return simulated_model, extended_validation_traject, F_ts_true, F_ts_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_results_from_cluster_until_ts(model_to_simulate, validation_trajectory_section, full_validation_trajectory, solve_ivp_method, dt, t_steps_beyond_cluster, threshold_for_ts, a0, model_type, C1=None, C2=None, C3=None, F_xlim = None, print_switch_index_val = True, disp_plot_file_name= None, vel_plot_file_name = None, force_plot_file_name= None, plot = False, save=False):\n",
    "#   Version 1: This function takes a model that has been trained and simulates it in time from the cluster from which it was found until a divergence time ts.\n",
    "# \n",
    "#   Parameters: model_to_simulate: Is the fit SINDy found model to simulate.\n",
    "#               validation_trajectory_section: Is the array single trajectory section within \"full_validation_trajectory\" that we can to use as validation for the simulated SINDy found model.\n",
    "#                                              Typically is a section of a cluster. Therefore, it must have a shape of (num_of_data, 3). (dist, vel, phase). Example: mult_traj_clusters_sections[traject][cluster_att][10]\n",
    "#               full_validation_trajectory: Is the complete trajectory starting from transient to steady state that can be used as validation. It is used to simulate a bit furhter than the cluster size. Example: x_train_DMT_mult_traj[traject]\n",
    "#               solve_ivp_method: Is a string to choose the method to perform the numerical integration like 'RK45' or 'BDF'.\n",
    "#               disp_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "#               force_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "\n",
    "    t_simulate = np.arange(validation_trajectory_section[0][2], validation_trajectory_section[0][2]+(t_steps_beyond_cluster*dt), dt) #This is the reason why it only works for attractive clusters so far. t_simulate can not go further than att region \"+2.5\"\n",
    "    init_cond = validation_trajectory_section[0]\n",
    "\n",
    "    simulated_model = model_to_simulate.simulate(init_cond, t_simulate, integrator_kws={\"method\": solve_ivp_method, \"rtol\": 1e-5, \"atol\": 1e-7})\n",
    "\n",
    "    init_cond_index = np.where(full_validation_trajectory[:,2] == t_simulate[0])[0][0] #Find where in the full trajectory the t_simulate starts for plotting and giving a score\n",
    "    next_index = init_cond_index + simulated_model.shape[0] #Find where in the full trajectory the t_simulate ends or plotting and giving a score\n",
    "\n",
    "    extended_validation_traject = np.vstack((full_validation_trajectory[init_cond_index:next_index,0], full_validation_trajectory[init_cond_index:next_index,1], full_validation_trajectory[init_cond_index:next_index,2])).T\n",
    "\n",
    "    # Extract time vector from validation_data (assuming it's the third column)\n",
    "    tvec = extended_validation_traject[:, 2]\n",
    "    t_simulate = simulated_model[:,2]\n",
    "\n",
    "    # Ensure the lengths match\n",
    "    if extended_validation_traject.shape != simulated_model.shape:\n",
    "        raise ValueError('Validation data and model predictions must have the same shape.' + str(extended_validation_traject.shape) + ' and ' + str(simulated_model.shape))\n",
    "\n",
    "    tlength, nterms = simulated_model.shape  # Time length and number of variables\n",
    "    abserror = np.abs(extended_validation_traject - simulated_model)  # Element-wise absolute error\n",
    "\n",
    "    # Initialize outputs\n",
    "    tdiv = np.zeros(nterms-1) \n",
    "    abserror_avg = np.zeros(nterms-1) #intentionally excluting time, as I do not need discrepancies in time\n",
    "    rmse = np.zeros(nterms-1)\n",
    "    abserror_over_time = np.zeros(nterms-1)\n",
    "\n",
    "    # Loop over each state variable\n",
    "    for state_variable in range(nterms-1): # why nterms-1?: intentionally excluting time, as I do not need discrepancies in time\n",
    "        # Calculate time-dependent squared error\n",
    "        time_diff = (extended_validation_traject[:, state_variable] - simulated_model[:, state_variable])**2\n",
    "        \n",
    "        # Compute cumulative RMS for divergence detection\n",
    "        rms = np.sqrt(np.cumsum(time_diff) / np.arange(1, len(time_diff) + 1))\n",
    "        \n",
    "        # Find the first index where RMS exceeds the threshold\n",
    "        if np.any(rms > threshold_for_ts):\n",
    "            switch_ind = np.argmax(rms > threshold_for_ts)\n",
    "        else:\n",
    "            switch_ind = len(tvec)  # No significant divergence detected\n",
    "        \n",
    "        # Record divergence time\n",
    "        tdiv[state_variable] = tvec[min(switch_ind, len(tvec) - 1)]  # Ensure bounds safety\n",
    "    if print_switch_index_val:    \n",
    "        print('rms was: ' + str(rms[-1]))\n",
    "        print('switch_ind was: ' + str(switch_ind))\n",
    "\n",
    "    if model_type == 'Normal DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2)\n",
    "        F_ts_true_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "    elif model_type == 'Viscoelastic DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT_viscoelast_damp(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2, C3=C3)\n",
    "        F_ts_true_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "\n",
    "    ts_time = t_simulate[:switch_ind]\n",
    "\n",
    "    if plot: \n",
    "        #Plots displacement:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(extended_validation_traject[:switch_ind,2], extended_validation_traject[:switch_ind,0], color = 'green', label = r'True $\\eta_1$')\n",
    "        axs.plot(t_simulate[:switch_ind], simulated_model[:switch_ind,0], '--', color = 'black', label = r'Found $\\eta_1$')\n",
    "        axs.plot(validation_trajectory_section[:,2], validation_trajectory_section[:,0], '-', color = 'red', linewidth = 3, alpha = 0.5, label = r'Cluster Extension')\n",
    "        axs.set_title('Comparison between true and found dynamics', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Displacement [-]', fontsize=25)\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick labels\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick label\n",
    "        if save:\n",
    "            plt.savefig(disp_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "        #Plots Velocity:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(extended_validation_traject[:switch_ind,2], extended_validation_traject[:switch_ind,1], color = 'green', label = r'True $\\eta_2$')\n",
    "        axs.plot(t_simulate[:switch_ind], simulated_model[:switch_ind,1], '--', color = 'black', label = r'Found $\\eta_2$')\n",
    "        axs.plot(validation_trajectory_section[:,2], validation_trajectory_section[:,1], '-', color = 'red', linewidth = 3, alpha = 0.5, label = r'Cluster Extension')\n",
    "        axs.set_title('Comparison between true and found dynamics', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Velocity [-]', fontsize=25)\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick labels\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick label\n",
    "        if save:\n",
    "            plt.savefig(vel_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "        #Checking parsimony:\n",
    "        eq_fund_tab_att_pareto = convert_to_table(model_to_simulate.equations(precision=9)[1])\n",
    "        eq_len_att_pareto = eq_fund_tab_att_pareto.count()\n",
    "\n",
    "        print('')\n",
    "        print('Equation with length: ' + str(eq_len_att_pareto))\n",
    "        model_to_simulate.print(precision=9)\n",
    "        print('')\n",
    "\n",
    "        # Plot the force calculations:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(t_simulate, F_ts_true, color='black', label=r'True $F_{ts}$')\n",
    "        axs.plot(t_simulate[:switch_ind], F_ts_true_sim[:switch_ind], '--', color='red', label=r'Found $F_{ts}$')\n",
    "        axs.set_title('Comparison between true and found force', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Tip-Sample Force [-]', fontsize=25)\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick labels\n",
    "\n",
    "        # Set the Y-axis to scientific notation\n",
    "        formatter = ScalarFormatter(useMathText=True)\n",
    "        formatter.set_scientific(True)\n",
    "        formatter.set_powerlimits((-2, 2))  # Specify the limits for scientific notation\n",
    "        axs.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "        if F_xlim is not None:\n",
    "            axs.set_xlim([F_xlim[0], F_xlim[-1]])\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "\n",
    "        if save:\n",
    "            plt.savefig(force_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "    \n",
    "    return simulated_model, extended_validation_traject, F_ts_true, F_ts_true_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7129e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_phase_space_from_most_freq_models_until_ts(model_to_simulate, validation_trajectory_section, full_validation_trajectory, solve_ivp_method, dt, t_steps_beyond_cluster, threshold_for_ts, a0, model_type, C1=None, C2=None, C3=None, disp_plot_file_name= None, vel_plot_file_name = None, force_plot_file_name= None, plot = False, save=False):\n",
    "#   Version 1: This function takes a model that has been trained and simulates it in time from the cluster from which it was found until a divergence criteria is met.\n",
    "#              It also shows the trajectory in the phase space \n",
    "# \n",
    "#   Parameters: model_to_simulate: Is the fit SINDy found model to simulate.\n",
    "#               validation_trajectory_section: Is the array single trajectory section within \"full_validation_trajectory\" that we can to use as validation for the simulated SINDy found model.\n",
    "#                                              Typically is a section of a cluster. Therefore, it must have a shape of (num_of_data, 3). (dist, vel, phase). Example: mult_traj_clusters_sections[traject][cluster_att][10]\n",
    "#               full_validation_trajectory: Is the complete trajectory starting from transient to steady state that can be used as validation. It is used to simulate a bit furhter than the cluster size. Example: x_train_DMT_mult_traj[traject]\n",
    "#               solve_ivp_method: Is a string to choose the method to perform the numerical integration like 'RK45' or 'BDF'.\n",
    "#               disp_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "#               force_plot_file_name: Is the name for the displacement figure if Save=True.\n",
    "\n",
    "    t_simulate = np.arange(validation_trajectory_section[0][2], validation_trajectory_section[0][2]+(t_steps_beyond_cluster*dt), dt) #This is the reason why it only works for attractive clusters so far. t_simulate can not go further than att region \"+2.5\"\n",
    "    init_cond = validation_trajectory_section[0]\n",
    "\n",
    "    simulated_model = model_to_simulate.simulate(init_cond, t_simulate, integrator_kws={\"method\": solve_ivp_method, \"rtol\": 1e-5, \"atol\": 1e-7})\n",
    "\n",
    "    init_cond_index = np.where(full_validation_trajectory[:,2] == t_simulate[0])[0][0] #Find where in the full trajectory the t_simulate starts for plotting and giving a score\n",
    "    next_index = init_cond_index + simulated_model.shape[0] #Find where in the full trajectory the t_simulate ends or plotting and giving a score\n",
    "\n",
    "    extended_validation_traject = np.vstack((full_validation_trajectory[init_cond_index:next_index,0], full_validation_trajectory[init_cond_index:next_index,1], full_validation_trajectory[init_cond_index:next_index,2])).T\n",
    "\n",
    "    # Extract time vector from validation_data (assuming it's the third column)\n",
    "    tvec = extended_validation_traject[:, 2]\n",
    "    t_simulate = simulated_model[:,2]\n",
    "\n",
    "    # Ensure the lengths match\n",
    "    if extended_validation_traject.shape != simulated_model.shape:\n",
    "        raise ValueError('Validation data and model predictions must have the same shape.' + str(extended_validation_traject.shape) + ' and ' + str(simulated_model.shape))\n",
    "\n",
    "    tlength, nterms = simulated_model.shape  # Time length and number of variables\n",
    "    abserror = np.abs(extended_validation_traject - simulated_model)  # Element-wise absolute error\n",
    "\n",
    "    # Initialize outputs\n",
    "    tdiv = np.zeros(nterms-1) \n",
    "    abserror_avg = np.zeros(nterms-1) #intentionally excluting time, as I do not need discrepancies in time\n",
    "    rmse = np.zeros(nterms-1)\n",
    "    abserror_over_time = np.zeros(nterms-1)\n",
    "\n",
    "    # Loop over each state variable\n",
    "    for state_variable in range(nterms-1): # why nterms-1?: intentionally excluting time, as I do not need discrepancies in time\n",
    "        # Calculate time-dependent squared error\n",
    "        time_diff = (extended_validation_traject[:, state_variable] - simulated_model[:, state_variable])**2\n",
    "        \n",
    "        # Compute cumulative RMS for divergence detection\n",
    "        rms = np.sqrt(np.cumsum(time_diff) / np.arange(1, len(time_diff) + 1))\n",
    "        \n",
    "        # Find the first index where RMS exceeds the threshold\n",
    "        if np.any(rms > threshold_for_ts):\n",
    "            switch_ind = np.argmax(rms > threshold_for_ts)\n",
    "        else:\n",
    "            switch_ind = len(tvec)  # No significant divergence detected\n",
    "        \n",
    "        # Record divergence time\n",
    "        tdiv[state_variable] = tvec[min(switch_ind, len(tvec) - 1)]  # Ensure bounds safety\n",
    "    print('rms was: ' + str(rms[-1]))\n",
    "    print('switch_ind was: ' + str(switch_ind))\n",
    "\n",
    "    simulated_model_until_ts = np.vstack((simulated_model[:switch_ind,0], simulated_model[:switch_ind,1], simulated_model[:switch_ind,2])).T\n",
    "\n",
    "    if model_type == 'Normal DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2)\n",
    "        F_ts_true_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "    elif model_type == 'Viscoelastic DMT':\n",
    "        F_ts_true = calculate_true_F_ts_DMT_viscoelast_damp(sim_disp_data=full_validation_trajectory[init_cond_index:next_index], a0=a0, C1=C1, C2=C2, C3=C3)\n",
    "        F_ts_true_sim = simulate_F_ts_from_found_model_DMT(model_to_simulate, simulated_model)\n",
    "\n",
    "    if plot: \n",
    "        #Plots displacement:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(full_validation_trajectory[-16000:,0], full_validation_trajectory[-16000:,1], color = 'green', label = r'True $\\eta_1$')\n",
    "        axs.plot(simulated_model[:switch_ind,0], simulated_model[:switch_ind,1], '--', color = 'black', linewidth = 2.5, label = r'Found $\\eta_1$')\n",
    "        axs.plot(validation_trajectory_section[:,0], validation_trajectory_section[:,1], '-', color = 'red', linewidth = 3, alpha = 0.5, label = r'Cluster Extension')\n",
    "        axs.set_title('Comparison between true and found dynamics', fontsize=25)\n",
    "        axs.set_xlabel(r'Time [-]', fontsize=25)\n",
    "        axs.set_ylabel(r'Displacement [-]', fontsize=25)\n",
    "        axs.legend(loc='best', fontsize=15)\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  \n",
    "        if save:\n",
    "            plt.savefig(disp_plot_file_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "    \n",
    "    return simulated_model_until_ts, extended_validation_traject, F_ts_true, F_ts_true_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e991da",
   "metadata": {},
   "source": [
    "## Functions to evaluate models with AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adef084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ts_and_RMSE_single_model_disp_vel_F_ts(validation_data, simulated_data, validation_force, simulated_force, threshold_for_ts, plot =False):\n",
    "#   Version 1: This function calculates divergence time, absolute error, average absolute error, and RMSE between model predictions and validation data.\n",
    "#\n",
    "#   Parameters: validation_data: np.ndarray Validation data (time-series matrix of shape [time, variables]).\n",
    "#               simulated_data: np.ndarray Model predictions (time-series matrix of the same shape as validation_data).\n",
    "\n",
    "    # Ensure inputs are numpy arrays\n",
    "    validation_data = np.asarray(validation_data)\n",
    "    simulated_data = np.asarray(simulated_data)\n",
    "\n",
    "    # Extract time vector from validation_data (third column)\n",
    "    tvec = validation_data[:, 2]\n",
    "    t_simulate = simulated_data[:,2]\n",
    "    \n",
    "    # Ensure the lengths match\n",
    "    if validation_data.shape != simulated_data.shape:\n",
    "        raise ValueError(\"Validation data and model predictions must have the same shape.\")\n",
    "    \n",
    "    tlength, nterms = simulated_data.shape  # Time length and number of variables\n",
    "    abserror = np.abs(validation_data - simulated_data)  # Element-wise absolute error\n",
    "\n",
    "    # Initialize outputs\n",
    "    tdiv = np.zeros(nterms-1) \n",
    "    abserror_avg = np.zeros(nterms-1) #intentionally excluting time, as discrepancies in time are not needed\n",
    "    rmse = np.zeros(nterms-1)\n",
    "    abserror_over_time = np.zeros(nterms-1)\n",
    "    abserror_over_time_F_ts = np.zeros(1)\n",
    "\n",
    "    # Loop over each state variable\n",
    "    for state_variable in range(nterms-1): # why nterms-1?: intentionally excluting time, as I do not need discrepancies in time\n",
    "        # Calculate time-dependent squared error\n",
    "        time_diff = (validation_data[:, state_variable] - simulated_data[:, state_variable])**2\n",
    "        \n",
    "        # Compute cumulative RMS for divergence detection\n",
    "        rms = np.sqrt(np.cumsum(time_diff) / np.arange(1, len(time_diff) + 1))\n",
    "        \n",
    "        # Find the first index where RMS exceeds the threshold\n",
    "        if np.any(rms > threshold_for_ts):\n",
    "            switch_ind = np.argmax(rms > threshold_for_ts)\n",
    "            \n",
    "        else:\n",
    "            switch_ind = len(tvec)  # No significant divergence detected\n",
    "\n",
    "        # Record divergence time\n",
    "        tdiv[state_variable] = tvec[min(switch_ind, len(tvec) - 1)]  # Ensure bounds safety\n",
    "\n",
    "        # Compute average absolute error and RMSE up to divergence point\n",
    "        abserror_over_time[state_variable] = np.sum((simulated_data[:switch_ind, state_variable]-validation_data[:switch_ind, state_variable])**2)/(np.mean(tdiv))\n",
    "        \n",
    "        E_avg = np.sum(abserror_over_time)/(nterms-1)\n",
    "        \n",
    "        rmse[state_variable] = np.sqrt(np.mean(time_diff[:switch_ind]))\n",
    "\n",
    "    abserror_over_time_F_ts = np.sum((simulated_force[:switch_ind]-validation_force[:switch_ind])**2)/(np.mean(tdiv))\n",
    "\n",
    "    E_avg_F_ts = np.sum(abserror_over_time_F_ts)/(1)   \n",
    "\n",
    "    if plot:\n",
    "        print('The value for t_s in e1 and e2 was respectfully: ' + str(tdiv)) \n",
    "        print('Validation data time goes until ' +str(validation_data[-1,2]))\n",
    "        print('Error information:')\n",
    "        print('The avg absolute error in e1 and e2 was respectfully: ' + str(abserror_avg))\n",
    "        print('The RMSE in e1 and e2 was respectfully: ' + str(rmse)) \n",
    "\n",
    "        #Plots displacement:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(validation_data[:switch_ind,2], validation_data[:switch_ind,0], color = 'green', label = r'True $\\eta_1$')\n",
    "        plt.plot(t_simulate[:switch_ind], simulated_data[:switch_ind,0], '--', color = 'black', label = r'Found $\\eta_1$')\n",
    "        plt.title('Comparison between true dynamics and found dynamics')\n",
    "        plt.xlabel('Time [-]')\n",
    "        plt.ylabel('Displacement [-]')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "        #Plots Velocity:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(validation_data[:switch_ind,2], validation_data[:switch_ind,1], color = 'green', label = r'True $\\eta_2$')\n",
    "        plt.plot(t_simulate[:switch_ind], simulated_data[:switch_ind,1], '--', color = 'black', label = r'Found $\\eta_2$')\n",
    "        plt.title('Comparison between true dynamics and found dynamics')\n",
    "        plt.xlabel('Time [-]')\n",
    "        plt.ylabel('Velocity [-]')\n",
    "        plt.legend(loc='best')\n",
    "\n",
    "    return tdiv, abserror, E_avg, E_avg_F_ts, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c51d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AIC_single_model_disp_vel_F_ts(E_avg, E_avg_F_ts, analyzed_model, N):\n",
    "#   Version 1: This function calculates Akaike Information Criterion (AIC), corrected AIC (AICc), and Bayesian Information Criterion (BIC) for a given model.\n",
    "#\n",
    "#   Parameters: E_avg: np.ndarray Vector of absolute errors between the model predictions and the validation data.\n",
    "#               E_avg_F_ts: np.ndarray Vector of absolute errors between the model predictions and the validation data for the force.\n",
    "#               analyzed_model: Model that produced the data that was used to calculate the abserror.\n",
    "#               N: Number of data points in the validation set.\n",
    "    \n",
    "    #Checking parsimony:\n",
    "    analyzed_eq_tab = convert_to_table(analyzed_model.equations(precision=9)[1])\n",
    "    p = analyzed_eq_tab.count()\n",
    "\n",
    "    if p == 0:\n",
    "        p = 1  \n",
    "\n",
    "    # Calculate the mean squared error (MSE)\n",
    "    mse = np.mean(E_avg**2)\n",
    "\n",
    "    # Log-likelihood (assuming normally distributed errors)\n",
    "    logL = -N * np.log(mse) / 2\n",
    "\n",
    "    # Akaike Information Criterion (AIC)\n",
    "    epsilon = 1e-12\n",
    "    aic = (2*p) + (N*np.log((E_avg + epsilon)/N)) + (N*np.log((E_avg_F_ts + epsilon)/N))\n",
    "\n",
    "    # Corrected AIC for small datasets    \n",
    "    aic_c = aic + (2 * p * (p + 1)) / (N - p - 1)\n",
    "\n",
    "    # Bayesian Information Criterion (BIC)\n",
    "    bic = -2 * logL + p * np.log(N)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    IC = {\n",
    "        'aic': aic,\n",
    "        'aic_c': aic_c,\n",
    "        'bic': bic\n",
    "    }\n",
    "\n",
    "    return IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cluster_get_E_avg_until_ts_n_calculate_AIC_unique_models(unique_models_df, validation_trajectory_section, full_validation_trajectory, t_steps_beyond_cluster, threshold_for_ts, solve_ivp_method, dt, a0, model_type, C1=None, C2=None, C3=None):\n",
    "\n",
    "    AIC_models_in_cluster = []\n",
    "\n",
    "    for model_to_study in tqdm(unique_models_df['Candidate Model'], desc=\"Processing models\", unit=\"model\"):\n",
    "\n",
    "        simulated_data, extended_val_trajec, F_ts_true, F_ts_sim = simulate_results_from_cluster(model_to_simulate = model_to_study,\n",
    "                                                    validation_trajectory_section = validation_trajectory_section,\n",
    "                                                    full_validation_trajectory = full_validation_trajectory,\n",
    "                                                    solve_ivp_method = solve_ivp_method, dt=dt, t_steps_beyond_cluster = t_steps_beyond_cluster,\n",
    "                                                    model_type=model_type, a0=a0, C1=C1, C2=C2, C3=C3, \n",
    "                                                    plot = False)\n",
    "\n",
    "        tdiv, abserror, abserror_avg, E_avg_F_ts, rmse = calc_ts_and_RMSE_single_model_disp_vel_F_ts(validation_data = extended_val_trajec,\n",
    "                                                    simulated_data = simulated_data, validation_force = F_ts_true, \n",
    "                                                    simulated_force = F_ts_sim, threshold_for_ts = threshold_for_ts,\n",
    "                                                    plot = False)\n",
    "\n",
    "        AIC = calculate_AIC_single_model_disp_vel_F_ts(E_avg = abserror_avg, E_avg_F_ts = E_avg_F_ts, analyzed_model = model_to_study, N = 1)\n",
    "        AIC_models_in_cluster.append(AIC)\n",
    "    unique_models_df['AIC_c'] = AIC_models_in_cluster\n",
    "\n",
    "    simulated_information = [simulated_data, extended_val_trajec, F_ts_true, F_ts_sim]\n",
    "    error_information = [tdiv, abserror, abserror_avg, E_avg_F_ts, rmse]\n",
    "\n",
    "    return simulated_information, error_information, AIC_models_in_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704eca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_AIC_c_score(AIC_score_models_in_cluster):\n",
    "    \n",
    "    min_AIC_c = np.inf\n",
    "    relative_AIC_c_list = []\n",
    "    for model in AIC_score_models_in_cluster:\n",
    "        if model['aic_c'] < min_AIC_c:\n",
    "            min_AIC_c = model['aic_c']\n",
    "    \n",
    "    for model in AIC_score_models_in_cluster:\n",
    "        rel_AIC_c = model['aic_c'] - min_AIC_c\n",
    "        relative_AIC_c_list.append(rel_AIC_c)\n",
    "        \n",
    "    return relative_AIC_c_list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f58a1",
   "metadata": {},
   "source": [
    "## Functions for statistical analysis of AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a18c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_active_cand_functions_from_found_eq(candidate_model):\n",
    "\n",
    "    # Get the feature names\n",
    "    candidate_functions = candidate_model.feature_library.get_feature_names()\n",
    "\n",
    "    # Get the coefficient matrix (Theta)\n",
    "    coefficients = candidate_model.coefficients()\n",
    "\n",
    "    active_features = []\n",
    "    for eq_coeffs in coefficients:\n",
    "        active_features.append([candidate_functions[i] for i in range(len(eq_coeffs)) if eq_coeffs[i] != 0])\n",
    "\n",
    "    return active_features[1] # to extract only features from e2' equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fb1df",
   "metadata": {},
   "source": [
    "## Functions to find Intermolecular distance value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d688296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intermolecular_distance(found_SINDy_model, test_trajectory, x_train_synthetic_mult_traj, analyzed_cluster, init_cluster_size, error_threshold, theoretical_a0, solve_ivp_method, image_name=None, plot_results = False, save=False):\n",
    "\n",
    "    x_test_regime_mult_traj = []\n",
    "    init_cond_to_simulate = []\n",
    "    a0_temp_list = []\n",
    "    error_list = []\n",
    "    error_temp = 1e-7\n",
    "\n",
    "    while error_temp < error_threshold: #1e-3\n",
    "        x_test_regime_mult_traj.clear()\n",
    "        init_cond_to_simulate.clear()\n",
    "        for i in analyzed_cluster:#[0]:\n",
    "            x_test_traject_temp = np.vstack((test_trajectory[i:(i+init_cluster_size),0], test_trajectory[i:(i+init_cluster_size),1], test_trajectory[i:(i+init_cluster_size),2])).T\n",
    "            x_test_regime_mult_traj.append(x_test_traject_temp)\n",
    "            init_cond_to_simulate.append(test_trajectory[i])\n",
    "        \n",
    "        found_SINDy_model_simulation = found_SINDy_model.simulate(init_cond_to_simulate[0], x_test_regime_mult_traj[0][:,2], integrator_kws={\"method\": solve_ivp_method, \"rtol\": 1e-5, \"atol\": 1e-7})\n",
    "        error_temp = (((found_SINDy_model_simulation[:,0][-1])-(x_test_regime_mult_traj[0][:,0][-1]))**2)+1*(((found_SINDy_model_simulation[:,1][-1])-(x_test_regime_mult_traj[0][:,1][-1]))**2)\n",
    "        error_list.append(error_temp)\n",
    "        a0_temp_list.append(x_test_regime_mult_traj[0][:,0][-1])\n",
    "        a0_temp = x_test_regime_mult_traj[0][:,0][-1]\n",
    "        init_cluster_size=init_cluster_size+1\n",
    "    \n",
    "    a0_estimation_error = abs(round(((a0_temp-(1-theoretical_a0))/(a0_temp))*100, 9))\n",
    "\n",
    "    if plot_results:\n",
    "        fig, axs = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        axs.plot(x_train_synthetic_mult_traj[0][:,0], x_train_synthetic_mult_traj[0][:,1], color='black', label='Test data', alpha=0.5, linewidth=1.8)\n",
    "        axs.plot(found_SINDy_model_simulation[:,0], found_SINDy_model_simulation[:,1], color='red', linestyle = '-', label='Simualted data') \n",
    "        axs.plot(x_test_regime_mult_traj[0][:,0], x_test_regime_mult_traj[0][:,1], color='blue', linestyle = '--', label='test data')\n",
    "        axs.set_xlabel(r'$\\bar{\\eta}_1$ (-)', fontsize=25)\n",
    "        axs.set_ylabel(r'$\\bar{\\eta}_2$ (-)', fontsize=25)\n",
    "        axs.set_title('Phase Space of DMT Model: Evolution of \\nUsing  Simulated data from a repulsive Cluster', fontsize=25)\n",
    "        axs.legend(loc='upper left', fontsize=15)\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick labels\n",
    "        axs.tick_params(axis='both', which='major', labelsize=25)  # Customize the font size of the tick label\n",
    "        if save:\n",
    "            plt.savefig(image_name + '.png', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "    print('a0 = ' +str(1-theoretical_a0))\n",
    "    print('Found a0 = ' +str(round(a0_temp, 7)))\n",
    "    print('The error in estimating a0 is: ' + str(abs(round(((a0_temp-(1-theoretical_a0))/(a0_temp))*100, 4))) + str(' %'))\n",
    "\n",
    "    return a0_temp, a0_temp_list, a0_estimation_error, error_list, found_SINDy_model_simulation, x_test_regime_mult_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100d85f",
   "metadata": {},
   "source": [
    "## Functions for plotting equation bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bb2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_n_compare_sindy_found_coefficients(original_coeffs, identified_coeffs):\n",
    "#   Version 1: Normalize identified coefficients by comparing to original ones\n",
    "#              - For true terms: ratio of identified to original.\n",
    "#              - For false positives: relative to largest true coefficient.\n",
    "#\n",
    "#   Parameters: original_coeffs: np.ndarray Array of coeeficients in the original dyanmics.\n",
    "#               identified_coeffs: np.ndarray Array of coeeficients in the found dyanmics.\n",
    "#\n",
    "#   Returns: Normalized array same shape as identified_coeffs\n",
    "\n",
    "    true_mask = original_coeffs != 0\n",
    "    true_nonzero_values = np.abs(original_coeffs[true_mask])\n",
    "    if true_nonzero_values.size == 0:\n",
    "        raise ValueError(\"No non-zero coefficients in original model.\")\n",
    "    max_true_coeff = np.max(true_nonzero_values)\n",
    "\n",
    "    normalized = np.zeros_like(identified_coeffs)\n",
    "\n",
    "    for i in range(original_coeffs.shape[0]):\n",
    "        for j in range(original_coeffs.shape[1]):\n",
    "            if true_mask[i, j]:\n",
    "                # Avoid division by zero (shouldnt happen if mask is correct)\n",
    "                if original_coeffs[i, j] != 0:\n",
    "                    normalized[i, j] = np.abs(identified_coeffs[i, j]) / np.abs(original_coeffs[i, j])\n",
    "            else:\n",
    "                normalized[i, j] = np.abs(identified_coeffs[i, j]) / max_true_coeff\n",
    "\n",
    "    return normalized\n",
    "\n",
    "np.random.seed(100)\n",
    "def plot_dynamics_bars(original_coeffs, identified_coeffs, plot_file_name, n_terms_correction=None, save_plot=False):\n",
    "    n_eqs, n_terms = original_coeffs.shape\n",
    "    normalized_identified = normalize_n_compare_sindy_found_coefficients(original_coeffs, identified_coeffs)\n",
    "\n",
    "    candidate_labels = [f\"$\\\\theta_{{{i+1}}}$\" for i in range(identified_coeffs.shape[1])]\n",
    "\n",
    "    fig, axs = plt.subplots(n_eqs, 1, figsize=(18, 2.4 * n_eqs), sharex=True)\n",
    "\n",
    "    if n_eqs == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    bar_width = 0.35\n",
    "    x = np.arange(n_terms)\n",
    "    \n",
    "    for i in range(n_eqs):\n",
    "        axs[i].bar(x - bar_width/2, (original_coeffs[i] != 0).astype(float), width=bar_width, label='Original Dynamics', color='black')\n",
    "        axs[i].bar(x + bar_width/2, normalized_identified[i], width=bar_width, label='Identified Dynamics', color='mediumseagreen')\n",
    "        axs[i].set_ylim(0, 1.1)\n",
    "        # axs[i].set_ylabel(f'$\\dot{{\\eta}}_{i+1}$' if i < 2 else '$\\dot{\\phi}$', fontsize=16)\n",
    "        \n",
    "        axs[i].spines['right'].set_visible(False)\n",
    "        axs[i].spines['top'].set_visible(False)\n",
    "        \n",
    "        # Increase axis line thickness\n",
    "        for spine in ['left', 'bottom']:\n",
    "            axs[i].spines[spine].set_linewidth(3)\n",
    "\n",
    "        axs[i].xaxis.set_major_locator(MaxNLocator(nbins=3)) \n",
    "        axs[i].set_yticks([0.33, 0.66, 1.0])\n",
    "\n",
    "        # Format y-axis numbers to one decimal place\n",
    "        axs[i].yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=29, width=2, length=6)\n",
    "\n",
    "        axs[i].grid(axis='y', linestyle='-', linewidth=1.0, color='gray', alpha=0.4)\n",
    "\n",
    "\n",
    "    if candidate_labels is None:\n",
    "        candidate_labels = [f\"$\\\\theta_{{{i+1}}}$\" for i in range(n_terms)]\n",
    "\n",
    "    axs[-1].set_xticks(x)\n",
    "    axs[-1].set_xticklabels(candidate_labels, rotation=0, ha='center', fontsize=35)\n",
    "    axs[-1].set_xlim(-1.0, (n_terms-n_terms_correction) - 0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_plot:\n",
    "        fig.savefig(f'{plot_file_name}.png', transparent=True, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84716f6b",
   "metadata": {},
   "source": [
    "## Accuracy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_predictions(true_values, predicted_values, label=\"Variable\"):\n",
    "    mse = mean_squared_error(true_values, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true_values, predicted_values)\n",
    "    \n",
    "    # Add normalized RMSE\n",
    "    value_range = np.max(true_values) - np.min(true_values)\n",
    "    nrmse = rmse / value_range if value_range != 0 else np.nan  # Avoid division by zero\n",
    "    \n",
    "    print(f\"--- {label} ---\")\n",
    "    print(f\"MSE:    {mse:.6f}\")\n",
    "    print(f\"RMSE:   {rmse:.6f}\")\n",
    "    print(f\"NRMSE:  {nrmse:.4%}\")\n",
    "    print(f\"R:     {r2:.6f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
